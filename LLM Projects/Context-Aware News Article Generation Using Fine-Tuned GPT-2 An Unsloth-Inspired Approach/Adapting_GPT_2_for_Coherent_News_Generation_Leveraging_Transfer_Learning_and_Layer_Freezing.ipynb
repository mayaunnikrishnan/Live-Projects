{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33e15833cc5a4a28b66788d96e420a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66b44dfc4ff24cf8be61d2533042f596",
              "IPY_MODEL_51cc3a3a19e14faf8b7d4a17ad70f681",
              "IPY_MODEL_7ed59faa1cfc4d08bbae1737b3a78307"
            ],
            "layout": "IPY_MODEL_db87daf315734ad7a862e091f0e22a3f"
          }
        },
        "66b44dfc4ff24cf8be61d2533042f596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41bc0cf7037941209110cfc15d30b19b",
            "placeholder": "​",
            "style": "IPY_MODEL_c4d2d88d27944dbfbf2efda1c015b36e",
            "value": "Map: 100%"
          }
        },
        "51cc3a3a19e14faf8b7d4a17ad70f681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_567d0bdaebb54f56a81feace42ec6562",
            "max": 69815,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0bfe4f93d84442db46b6f6b50f855ff",
            "value": 69815
          }
        },
        "7ed59faa1cfc4d08bbae1737b3a78307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f7efa74ec324460bfc9f5d0ebb0d112",
            "placeholder": "​",
            "style": "IPY_MODEL_890c109df01e4c3e96181f8f63902ab1",
            "value": " 69815/69815 [00:25&lt;00:00, 2948.04 examples/s]"
          }
        },
        "db87daf315734ad7a862e091f0e22a3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bc0cf7037941209110cfc15d30b19b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d2d88d27944dbfbf2efda1c015b36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "567d0bdaebb54f56a81feace42ec6562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0bfe4f93d84442db46b6f6b50f855ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f7efa74ec324460bfc9f5d0ebb0d112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890c109df01e4c3e96181f8f63902ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc74b09baab844daabacfcaae1d34557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b4175f943b7492d8ca6de67b2799752",
              "IPY_MODEL_3d3bab2f01f2457ebc56f1deb66ab119",
              "IPY_MODEL_bcbbdcedcb7e4b858643a3f14166762f"
            ],
            "layout": "IPY_MODEL_79086e740f9140d5a2197d13b8e29dae"
          }
        },
        "2b4175f943b7492d8ca6de67b2799752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c555c27db3641a996945b7edbc1f5dc",
            "placeholder": "​",
            "style": "IPY_MODEL_eec7d40cd7aa402c9c9ec1b0f1ac1e3d",
            "value": "Map: 100%"
          }
        },
        "3d3bab2f01f2457ebc56f1deb66ab119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f1d29ab76354e7ca8a9c41afa26dbec",
            "max": 7758,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b636d8bc052345f4a394edf1a4445950",
            "value": 7758
          }
        },
        "bcbbdcedcb7e4b858643a3f14166762f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae19575f2324416ca42a00b4fe663a7d",
            "placeholder": "​",
            "style": "IPY_MODEL_b8b2eb42f5884c02879bce2c67f88642",
            "value": " 7758/7758 [00:03&lt;00:00, 2369.98 examples/s]"
          }
        },
        "79086e740f9140d5a2197d13b8e29dae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c555c27db3641a996945b7edbc1f5dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eec7d40cd7aa402c9c9ec1b0f1ac1e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f1d29ab76354e7ca8a9c41afa26dbec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b636d8bc052345f4a394edf1a4445950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae19575f2324416ca42a00b4fe663a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8b2eb42f5884c02879bce2c67f88642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xRJ_iM_Kg7S",
        "outputId": "73fd42a2-cc1a-487c-bcd4-1f4c22f736d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==10.0.1\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==10.0.1) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.7.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 10.0.1 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.16.1 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15 pyarrow-10.0.1 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyarrow==10.0.1 datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7svqQlxLU0R",
        "outputId": "e8e940c6-7714-48e2-e5ee-5ef8e5d8dcef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TrainerCallback\n",
        "from datasets import Dataset\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the padding token to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Freeze all layers except the last few\n",
        "for name, param in model.named_parameters():\n",
        "    if \"h.10\" not in name and \"h.11\" not in name:  # Assuming GPT-2 has 12 layers\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_and_preprocess_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['text'] = df['abstract']\n",
        "    dataset = Dataset.from_pandas(df[['text']])\n",
        "    return dataset\n",
        "\n",
        "dataset_path = 'cleaned_NYT_2016_to_2022-updated.csv'\n",
        "dataset = load_and_preprocess_dataset(dataset_path)\n",
        "\n",
        "# Split the dataset\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Define a function for data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    eval_strategy=\"steps\",  # Use eval_strategy instead of evaluation_strategy\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    per_device_train_batch_size=16,  # Adjust based on your GPU's memory\n",
        "    gradient_accumulation_steps=1,  # No gradient accumulation to maximize GPU memory usage\n",
        "    num_train_epochs=3,  # Fine-tune for more epochs since only a few layers are updated\n",
        "    weight_decay=0.01,\n",
        "    logging_first_step=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),  # Enable mixed precision if on GPU\n",
        ")\n",
        "\n",
        "# Define a custom callback to monitor progress\n",
        "class CustomCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            print(logs)\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[CustomCallback()],  # Add custom callback for live printing\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "33e15833cc5a4a28b66788d96e420a40",
            "66b44dfc4ff24cf8be61d2533042f596",
            "51cc3a3a19e14faf8b7d4a17ad70f681",
            "7ed59faa1cfc4d08bbae1737b3a78307",
            "db87daf315734ad7a862e091f0e22a3f",
            "41bc0cf7037941209110cfc15d30b19b",
            "c4d2d88d27944dbfbf2efda1c015b36e",
            "567d0bdaebb54f56a81feace42ec6562",
            "a0bfe4f93d84442db46b6f6b50f855ff",
            "8f7efa74ec324460bfc9f5d0ebb0d112",
            "890c109df01e4c3e96181f8f63902ab1",
            "cc74b09baab844daabacfcaae1d34557",
            "2b4175f943b7492d8ca6de67b2799752",
            "3d3bab2f01f2457ebc56f1deb66ab119",
            "bcbbdcedcb7e4b858643a3f14166762f",
            "79086e740f9140d5a2197d13b8e29dae",
            "8c555c27db3641a996945b7edbc1f5dc",
            "eec7d40cd7aa402c9c9ec1b0f1ac1e3d",
            "0f1d29ab76354e7ca8a9c41afa26dbec",
            "b636d8bc052345f4a394edf1a4445950",
            "ae19575f2324416ca42a00b4fe663a7d",
            "b8b2eb42f5884c02879bce2c67f88642"
          ]
        },
        "id": "dFQuThAeK5_u",
        "outputId": "fe435931-8d17-4c24-861d-ba66385306f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/69815 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33e15833cc5a4a28b66788d96e420a40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7758 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc74b09baab844daabacfcaae1d34557"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2731' max='13092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2731/13092 11:41:36 < 44:23:43, 0.06 it/s, Epoch 0.63/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.499100</td>\n",
              "      <td>4.080737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.522000</td>\n",
              "      <td>4.003520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.430200</td>\n",
              "      <td>3.956403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>4.257600</td>\n",
              "      <td>3.924631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.266700</td>\n",
              "      <td>3.896185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>4.205600</td>\n",
              "      <td>3.874353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>4.279500</td>\n",
              "      <td>3.856035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>4.083800</td>\n",
              "      <td>3.841216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>4.145200</td>\n",
              "      <td>3.828338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.216000</td>\n",
              "      <td>3.816760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>4.195100</td>\n",
              "      <td>3.807758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>4.102900</td>\n",
              "      <td>3.797611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>4.075300</td>\n",
              "      <td>3.787426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>4.168200</td>\n",
              "      <td>3.781788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.149500</td>\n",
              "      <td>3.774863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>4.173400</td>\n",
              "      <td>3.768457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>4.114600</td>\n",
              "      <td>3.762523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>4.091800</td>\n",
              "      <td>3.757561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>4.100300</td>\n",
              "      <td>3.751068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.090700</td>\n",
              "      <td>3.745422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>4.060900</td>\n",
              "      <td>3.741356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>4.168300</td>\n",
              "      <td>3.737709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>4.078300</td>\n",
              "      <td>3.735475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>4.287300</td>\n",
              "      <td>3.733086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>4.037300</td>\n",
              "      <td>3.729512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>4.128000</td>\n",
              "      <td>3.725872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>4.018800</td>\n",
              "      <td>3.723411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>3.991200</td>\n",
              "      <td>3.719648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>4.064900</td>\n",
              "      <td>3.717849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.017800</td>\n",
              "      <td>3.714733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>3.946700</td>\n",
              "      <td>3.712759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>3.960900</td>\n",
              "      <td>3.712133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>4.065500</td>\n",
              "      <td>3.710698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>4.062800</td>\n",
              "      <td>3.708871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.909300</td>\n",
              "      <td>3.706191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>3.957000</td>\n",
              "      <td>3.703945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>4.023400</td>\n",
              "      <td>3.701684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>4.209200</td>\n",
              "      <td>3.700318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>4.020700</td>\n",
              "      <td>3.698971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>4.041700</td>\n",
              "      <td>3.697391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>3.880300</td>\n",
              "      <td>3.696338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>3.885800</td>\n",
              "      <td>3.694119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>3.989500</td>\n",
              "      <td>3.692706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>4.075300</td>\n",
              "      <td>3.691515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.944100</td>\n",
              "      <td>3.690547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>4.080600</td>\n",
              "      <td>3.689513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>4.114000</td>\n",
              "      <td>3.688713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>4.022200</td>\n",
              "      <td>3.686986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>3.990800</td>\n",
              "      <td>3.684011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.045700</td>\n",
              "      <td>3.682122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>3.952100</td>\n",
              "      <td>3.681333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>4.033900</td>\n",
              "      <td>3.680978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>3.915000</td>\n",
              "      <td>3.680309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>3.997700</td>\n",
              "      <td>3.679340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>4.010000</td>\n",
              "      <td>3.677698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>4.010200</td>\n",
              "      <td>3.675269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>3.975700</td>\n",
              "      <td>3.674533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>4.000900</td>\n",
              "      <td>3.673156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>3.996200</td>\n",
              "      <td>3.671646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>4.042200</td>\n",
              "      <td>3.670777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>4.007800</td>\n",
              "      <td>3.669992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>4.055200</td>\n",
              "      <td>3.669285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>3.901800</td>\n",
              "      <td>3.669014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>3.948700</td>\n",
              "      <td>3.668294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>4.162200</td>\n",
              "      <td>3.667025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>4.071500</td>\n",
              "      <td>3.667295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>3.984600</td>\n",
              "      <td>3.666682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>3.971100</td>\n",
              "      <td>3.665922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>3.967800</td>\n",
              "      <td>3.663331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>3.926500</td>\n",
              "      <td>3.660988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>3.886500</td>\n",
              "      <td>3.659696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>3.889100</td>\n",
              "      <td>3.658680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>3.939700</td>\n",
              "      <td>3.658324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>3.979100</td>\n",
              "      <td>3.658442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>3.925500</td>\n",
              "      <td>3.658207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>3.971300</td>\n",
              "      <td>3.657941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>3.938700</td>\n",
              "      <td>3.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>3.929400</td>\n",
              "      <td>3.654676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>3.950300</td>\n",
              "      <td>3.654283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.929400</td>\n",
              "      <td>3.653232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>3.941000</td>\n",
              "      <td>3.652141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>4.057100</td>\n",
              "      <td>3.651856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>4.004400</td>\n",
              "      <td>3.651653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>3.957000</td>\n",
              "      <td>3.650965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>3.881400</td>\n",
              "      <td>3.650004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>3.922000</td>\n",
              "      <td>3.648515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>3.973600</td>\n",
              "      <td>3.647079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>3.864600</td>\n",
              "      <td>3.646208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>3.932500</td>\n",
              "      <td>3.645470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>3.999000</td>\n",
              "      <td>3.644653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>3.870700</td>\n",
              "      <td>3.644158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>3.973900</td>\n",
              "      <td>3.643768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>4.114100</td>\n",
              "      <td>3.642963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>3.958700</td>\n",
              "      <td>3.642379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>3.981500</td>\n",
              "      <td>3.641495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>3.995200</td>\n",
              "      <td>3.641283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>3.878100</td>\n",
              "      <td>3.641371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>3.922200</td>\n",
              "      <td>3.640347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>3.979600</td>\n",
              "      <td>3.639884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.953700</td>\n",
              "      <td>3.639917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>3.988700</td>\n",
              "      <td>3.639410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>3.923000</td>\n",
              "      <td>3.638015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>3.907400</td>\n",
              "      <td>3.637539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>3.909800</td>\n",
              "      <td>3.636538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>3.944000</td>\n",
              "      <td>3.635261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>4.035800</td>\n",
              "      <td>3.634406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>3.909600</td>\n",
              "      <td>3.633623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>4.005800</td>\n",
              "      <td>3.632500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>4.006400</td>\n",
              "      <td>3.632227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>4.023600</td>\n",
              "      <td>3.631586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>3.963600</td>\n",
              "      <td>3.630777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>3.947900</td>\n",
              "      <td>3.630789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>3.898600</td>\n",
              "      <td>3.630836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>3.856300</td>\n",
              "      <td>3.630782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>4.027400</td>\n",
              "      <td>3.630554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>3.924500</td>\n",
              "      <td>3.630123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>3.988300</td>\n",
              "      <td>3.629789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>3.984700</td>\n",
              "      <td>3.628893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>3.944100</td>\n",
              "      <td>3.628326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.998900</td>\n",
              "      <td>3.627520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>3.929000</td>\n",
              "      <td>3.627391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>4.023500</td>\n",
              "      <td>3.627598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>3.977700</td>\n",
              "      <td>3.627272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>3.869700</td>\n",
              "      <td>3.626498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>3.933800</td>\n",
              "      <td>3.625510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>3.834500</td>\n",
              "      <td>3.624234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>3.939500</td>\n",
              "      <td>3.623691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>3.874200</td>\n",
              "      <td>3.623625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>3.886900</td>\n",
              "      <td>3.622834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>3.956700</td>\n",
              "      <td>3.622865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>3.918800</td>\n",
              "      <td>3.622845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>3.863200</td>\n",
              "      <td>3.622684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>3.892100</td>\n",
              "      <td>3.621853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>3.899100</td>\n",
              "      <td>3.621921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>3.876600</td>\n",
              "      <td>3.621998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>3.945400</td>\n",
              "      <td>3.621618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>3.926600</td>\n",
              "      <td>3.620962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>3.888300</td>\n",
              "      <td>3.620859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>3.931000</td>\n",
              "      <td>3.620509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>3.863000</td>\n",
              "      <td>3.619895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>3.898000</td>\n",
              "      <td>3.618624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>3.864100</td>\n",
              "      <td>3.617491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>3.959400</td>\n",
              "      <td>3.617082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>3.903100</td>\n",
              "      <td>3.616568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>3.933800</td>\n",
              "      <td>3.615773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>3.859200</td>\n",
              "      <td>3.614831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>3.882700</td>\n",
              "      <td>3.614576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>4.031200</td>\n",
              "      <td>3.614494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>4.022300</td>\n",
              "      <td>3.614312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.928200</td>\n",
              "      <td>3.614548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>3.849600</td>\n",
              "      <td>3.613992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>3.945700</td>\n",
              "      <td>3.613683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>3.902900</td>\n",
              "      <td>3.613455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>3.951700</td>\n",
              "      <td>3.612751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>3.908200</td>\n",
              "      <td>3.611871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>3.852300</td>\n",
              "      <td>3.611298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>3.987300</td>\n",
              "      <td>3.611179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>3.960300</td>\n",
              "      <td>3.611422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>3.912200</td>\n",
              "      <td>3.612034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>3.909700</td>\n",
              "      <td>3.611957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>3.867900</td>\n",
              "      <td>3.612209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>3.875100</td>\n",
              "      <td>3.612031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1630</td>\n",
              "      <td>3.755500</td>\n",
              "      <td>3.611256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>3.896400</td>\n",
              "      <td>3.610436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>3.889200</td>\n",
              "      <td>3.609559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>3.851900</td>\n",
              "      <td>3.609485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1670</td>\n",
              "      <td>3.799200</td>\n",
              "      <td>3.609511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>3.889300</td>\n",
              "      <td>3.609685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1690</td>\n",
              "      <td>3.919200</td>\n",
              "      <td>3.609288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>3.935200</td>\n",
              "      <td>3.609026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1710</td>\n",
              "      <td>3.848900</td>\n",
              "      <td>3.608776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>3.943100</td>\n",
              "      <td>3.607674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1730</td>\n",
              "      <td>3.845400</td>\n",
              "      <td>3.606751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>3.847200</td>\n",
              "      <td>3.606741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>3.854000</td>\n",
              "      <td>3.606288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>3.871200</td>\n",
              "      <td>3.606326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>3.863100</td>\n",
              "      <td>3.605891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>3.970000</td>\n",
              "      <td>3.605829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1790</td>\n",
              "      <td>3.943400</td>\n",
              "      <td>3.604891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.975100</td>\n",
              "      <td>3.603893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1810</td>\n",
              "      <td>3.891500</td>\n",
              "      <td>3.603573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>3.960200</td>\n",
              "      <td>3.603730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1830</td>\n",
              "      <td>3.930000</td>\n",
              "      <td>3.603176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>3.852300</td>\n",
              "      <td>3.602538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>3.991800</td>\n",
              "      <td>3.601805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>3.972100</td>\n",
              "      <td>3.601225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>3.849500</td>\n",
              "      <td>3.600703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>3.876200</td>\n",
              "      <td>3.599891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1890</td>\n",
              "      <td>3.926600</td>\n",
              "      <td>3.600001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>3.802900</td>\n",
              "      <td>3.600342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1910</td>\n",
              "      <td>3.917000</td>\n",
              "      <td>3.600474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>3.855900</td>\n",
              "      <td>3.599968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1930</td>\n",
              "      <td>3.881600</td>\n",
              "      <td>3.600103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>3.870700</td>\n",
              "      <td>3.600077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>3.745400</td>\n",
              "      <td>3.599465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>3.866300</td>\n",
              "      <td>3.598552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1970</td>\n",
              "      <td>3.853700</td>\n",
              "      <td>3.597338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>3.867000</td>\n",
              "      <td>3.596715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990</td>\n",
              "      <td>3.886500</td>\n",
              "      <td>3.596128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.918900</td>\n",
              "      <td>3.596143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2010</td>\n",
              "      <td>3.948000</td>\n",
              "      <td>3.596296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2020</td>\n",
              "      <td>3.924900</td>\n",
              "      <td>3.596287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2030</td>\n",
              "      <td>3.911100</td>\n",
              "      <td>3.595846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2040</td>\n",
              "      <td>3.827100</td>\n",
              "      <td>3.596154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>3.896600</td>\n",
              "      <td>3.596719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2060</td>\n",
              "      <td>3.886600</td>\n",
              "      <td>3.597149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2070</td>\n",
              "      <td>3.993800</td>\n",
              "      <td>3.596982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2080</td>\n",
              "      <td>3.869400</td>\n",
              "      <td>3.596322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2090</td>\n",
              "      <td>3.967100</td>\n",
              "      <td>3.595712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>3.858800</td>\n",
              "      <td>3.595387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2110</td>\n",
              "      <td>3.830200</td>\n",
              "      <td>3.594435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2120</td>\n",
              "      <td>3.961100</td>\n",
              "      <td>3.594407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2130</td>\n",
              "      <td>3.887200</td>\n",
              "      <td>3.594195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2140</td>\n",
              "      <td>3.858300</td>\n",
              "      <td>3.594204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>3.851800</td>\n",
              "      <td>3.593762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2160</td>\n",
              "      <td>3.896900</td>\n",
              "      <td>3.593079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2170</td>\n",
              "      <td>3.951600</td>\n",
              "      <td>3.592376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2180</td>\n",
              "      <td>3.934100</td>\n",
              "      <td>3.592647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2190</td>\n",
              "      <td>3.938400</td>\n",
              "      <td>3.592405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>3.843500</td>\n",
              "      <td>3.591371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2210</td>\n",
              "      <td>3.914900</td>\n",
              "      <td>3.590770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2220</td>\n",
              "      <td>3.925300</td>\n",
              "      <td>3.590425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2230</td>\n",
              "      <td>3.842100</td>\n",
              "      <td>3.590308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2240</td>\n",
              "      <td>3.894300</td>\n",
              "      <td>3.590594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>3.940600</td>\n",
              "      <td>3.590809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2260</td>\n",
              "      <td>3.907900</td>\n",
              "      <td>3.591091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2270</td>\n",
              "      <td>3.934000</td>\n",
              "      <td>3.590422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2280</td>\n",
              "      <td>3.822400</td>\n",
              "      <td>3.589530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2290</td>\n",
              "      <td>4.062700</td>\n",
              "      <td>3.588827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>3.899000</td>\n",
              "      <td>3.587872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>3.872300</td>\n",
              "      <td>3.587328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2320</td>\n",
              "      <td>3.852100</td>\n",
              "      <td>3.587305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2330</td>\n",
              "      <td>3.951700</td>\n",
              "      <td>3.587280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2340</td>\n",
              "      <td>3.938400</td>\n",
              "      <td>3.587165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>3.889800</td>\n",
              "      <td>3.587410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>3.979500</td>\n",
              "      <td>3.587883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2370</td>\n",
              "      <td>3.932400</td>\n",
              "      <td>3.587594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2380</td>\n",
              "      <td>3.907500</td>\n",
              "      <td>3.587203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2390</td>\n",
              "      <td>3.951400</td>\n",
              "      <td>3.586843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>3.822300</td>\n",
              "      <td>3.586386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2410</td>\n",
              "      <td>3.872000</td>\n",
              "      <td>3.585867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2420</td>\n",
              "      <td>3.840500</td>\n",
              "      <td>3.586259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2430</td>\n",
              "      <td>3.733400</td>\n",
              "      <td>3.586512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2440</td>\n",
              "      <td>3.958200</td>\n",
              "      <td>3.586372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>3.807400</td>\n",
              "      <td>3.585927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2460</td>\n",
              "      <td>3.803600</td>\n",
              "      <td>3.585715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2470</td>\n",
              "      <td>3.862000</td>\n",
              "      <td>3.586043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2480</td>\n",
              "      <td>3.944100</td>\n",
              "      <td>3.586113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2490</td>\n",
              "      <td>3.852100</td>\n",
              "      <td>3.586488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.777600</td>\n",
              "      <td>3.586827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2510</td>\n",
              "      <td>3.896900</td>\n",
              "      <td>3.586926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2520</td>\n",
              "      <td>3.955200</td>\n",
              "      <td>3.587301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2530</td>\n",
              "      <td>3.917800</td>\n",
              "      <td>3.586904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2540</td>\n",
              "      <td>3.913900</td>\n",
              "      <td>3.586425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>3.827100</td>\n",
              "      <td>3.586014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2560</td>\n",
              "      <td>3.898300</td>\n",
              "      <td>3.585495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2570</td>\n",
              "      <td>3.871700</td>\n",
              "      <td>3.585147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2580</td>\n",
              "      <td>3.840400</td>\n",
              "      <td>3.585191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2590</td>\n",
              "      <td>3.927500</td>\n",
              "      <td>3.585042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>3.860000</td>\n",
              "      <td>3.584758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2610</td>\n",
              "      <td>3.986100</td>\n",
              "      <td>3.584404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2620</td>\n",
              "      <td>3.841100</td>\n",
              "      <td>3.583819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2630</td>\n",
              "      <td>3.796900</td>\n",
              "      <td>3.583670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2640</td>\n",
              "      <td>3.987600</td>\n",
              "      <td>3.582702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>3.925000</td>\n",
              "      <td>3.582214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2660</td>\n",
              "      <td>3.966400</td>\n",
              "      <td>3.581928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2670</td>\n",
              "      <td>3.879200</td>\n",
              "      <td>3.582236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2680</td>\n",
              "      <td>3.886100</td>\n",
              "      <td>3.582909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2690</td>\n",
              "      <td>3.946700</td>\n",
              "      <td>3.582639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>3.814400</td>\n",
              "      <td>3.582944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2710</td>\n",
              "      <td>3.943600</td>\n",
              "      <td>3.583202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2720</td>\n",
              "      <td>3.838700</td>\n",
              "      <td>3.582610</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 68/970 00:10 < 02:19, 6.47 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 4.8162, 'grad_norm': 1.22317636013031, 'learning_rate': 4.9996180873816074e-05, 'epoch': 0.00022914757103574703}\n",
            "{'loss': 4.4991, 'grad_norm': 1.1494336128234863, 'learning_rate': 4.996180873816071e-05, 'epoch': 0.00229147571035747}\n",
            "{'eval_loss': 4.0807366371154785, 'eval_runtime': 150.0016, 'eval_samples_per_second': 51.719, 'eval_steps_per_second': 6.467, 'epoch': 0.00229147571035747}\n",
            "{'loss': 4.522, 'grad_norm': 1.028442621231079, 'learning_rate': 4.992361747632142e-05, 'epoch': 0.00458295142071494}\n",
            "{'eval_loss': 4.003520488739014, 'eval_runtime': 150.0077, 'eval_samples_per_second': 51.717, 'eval_steps_per_second': 6.466, 'epoch': 0.00458295142071494}\n",
            "{'loss': 4.4302, 'grad_norm': 1.0079808235168457, 'learning_rate': 4.9885426214482125e-05, 'epoch': 0.0068744271310724105}\n",
            "{'eval_loss': 3.9564032554626465, 'eval_runtime': 150.416, 'eval_samples_per_second': 51.577, 'eval_steps_per_second': 6.449, 'epoch': 0.0068744271310724105}\n",
            "{'loss': 4.2576, 'grad_norm': 0.9909520149230957, 'learning_rate': 4.984723495264284e-05, 'epoch': 0.00916590284142988}\n",
            "{'eval_loss': 3.924630641937256, 'eval_runtime': 150.2794, 'eval_samples_per_second': 51.624, 'eval_steps_per_second': 6.455, 'epoch': 0.00916590284142988}\n",
            "{'loss': 4.2667, 'grad_norm': 1.0628306865692139, 'learning_rate': 4.980904369080355e-05, 'epoch': 0.011457378551787351}\n",
            "{'eval_loss': 3.8961853981018066, 'eval_runtime': 150.2801, 'eval_samples_per_second': 51.624, 'eval_steps_per_second': 6.455, 'epoch': 0.011457378551787351}\n",
            "{'loss': 4.2056, 'grad_norm': 0.9619551301002502, 'learning_rate': 4.9770852428964254e-05, 'epoch': 0.013748854262144821}\n",
            "{'eval_loss': 3.8743529319763184, 'eval_runtime': 150.4132, 'eval_samples_per_second': 51.578, 'eval_steps_per_second': 6.449, 'epoch': 0.013748854262144821}\n",
            "{'loss': 4.2795, 'grad_norm': 1.3455026149749756, 'learning_rate': 4.9732661167124966e-05, 'epoch': 0.016040329972502293}\n",
            "{'eval_loss': 3.8560352325439453, 'eval_runtime': 150.4435, 'eval_samples_per_second': 51.568, 'eval_steps_per_second': 6.448, 'epoch': 0.016040329972502293}\n",
            "{'loss': 4.0838, 'grad_norm': 1.067821741104126, 'learning_rate': 4.969446990528567e-05, 'epoch': 0.01833180568285976}\n",
            "{'eval_loss': 3.8412158489227295, 'eval_runtime': 150.5437, 'eval_samples_per_second': 51.533, 'eval_steps_per_second': 6.443, 'epoch': 0.01833180568285976}\n",
            "{'loss': 4.1452, 'grad_norm': 1.0300712585449219, 'learning_rate': 4.965627864344638e-05, 'epoch': 0.020623281393217233}\n",
            "{'eval_loss': 3.828338384628296, 'eval_runtime': 150.4807, 'eval_samples_per_second': 51.555, 'eval_steps_per_second': 6.446, 'epoch': 0.020623281393217233}\n",
            "{'loss': 4.216, 'grad_norm': 0.9672271609306335, 'learning_rate': 4.9618087381607096e-05, 'epoch': 0.022914757103574702}\n",
            "{'eval_loss': 3.8167598247528076, 'eval_runtime': 150.3842, 'eval_samples_per_second': 51.588, 'eval_steps_per_second': 6.45, 'epoch': 0.022914757103574702}\n",
            "{'loss': 4.1951, 'grad_norm': 1.1399493217468262, 'learning_rate': 4.95798961197678e-05, 'epoch': 0.025206232813932174}\n",
            "{'eval_loss': 3.807757616043091, 'eval_runtime': 150.5154, 'eval_samples_per_second': 51.543, 'eval_steps_per_second': 6.445, 'epoch': 0.025206232813932174}\n",
            "{'loss': 4.1029, 'grad_norm': 1.012883186340332, 'learning_rate': 4.9541704857928506e-05, 'epoch': 0.027497708524289642}\n",
            "{'eval_loss': 3.7976109981536865, 'eval_runtime': 150.3557, 'eval_samples_per_second': 51.598, 'eval_steps_per_second': 6.451, 'epoch': 0.027497708524289642}\n",
            "{'loss': 4.0753, 'grad_norm': 1.0553560256958008, 'learning_rate': 4.950351359608922e-05, 'epoch': 0.029789184234647114}\n",
            "{'eval_loss': 3.7874257564544678, 'eval_runtime': 150.3989, 'eval_samples_per_second': 51.583, 'eval_steps_per_second': 6.45, 'epoch': 0.029789184234647114}\n",
            "{'loss': 4.1682, 'grad_norm': 1.0590828657150269, 'learning_rate': 4.9465322334249923e-05, 'epoch': 0.032080659945004586}\n",
            "{'eval_loss': 3.7817881107330322, 'eval_runtime': 150.3574, 'eval_samples_per_second': 51.597, 'eval_steps_per_second': 6.451, 'epoch': 0.032080659945004586}\n",
            "{'loss': 4.1495, 'grad_norm': 1.0029064416885376, 'learning_rate': 4.9427131072410636e-05, 'epoch': 0.034372135655362054}\n",
            "{'eval_loss': 3.77486252784729, 'eval_runtime': 150.2995, 'eval_samples_per_second': 51.617, 'eval_steps_per_second': 6.454, 'epoch': 0.034372135655362054}\n",
            "{'loss': 4.1734, 'grad_norm': 1.1342052221298218, 'learning_rate': 4.938893981057134e-05, 'epoch': 0.03666361136571952}\n",
            "{'eval_loss': 3.7684566974639893, 'eval_runtime': 150.272, 'eval_samples_per_second': 51.626, 'eval_steps_per_second': 6.455, 'epoch': 0.03666361136571952}\n",
            "{'loss': 4.1146, 'grad_norm': 1.0345851182937622, 'learning_rate': 4.935074854873205e-05, 'epoch': 0.03895508707607699}\n",
            "{'eval_loss': 3.7625229358673096, 'eval_runtime': 150.2575, 'eval_samples_per_second': 51.631, 'eval_steps_per_second': 6.456, 'epoch': 0.03895508707607699}\n",
            "{'loss': 4.0918, 'grad_norm': 0.869449257850647, 'learning_rate': 4.9312557286892765e-05, 'epoch': 0.04124656278643447}\n",
            "{'eval_loss': 3.7575607299804688, 'eval_runtime': 150.3546, 'eval_samples_per_second': 51.598, 'eval_steps_per_second': 6.451, 'epoch': 0.04124656278643447}\n",
            "{'loss': 4.1003, 'grad_norm': 1.0223876237869263, 'learning_rate': 4.927436602505347e-05, 'epoch': 0.043538038496791935}\n",
            "{'eval_loss': 3.751067876815796, 'eval_runtime': 150.3078, 'eval_samples_per_second': 51.614, 'eval_steps_per_second': 6.453, 'epoch': 0.043538038496791935}\n",
            "{'loss': 4.0907, 'grad_norm': 1.001566767692566, 'learning_rate': 4.9236174763214175e-05, 'epoch': 0.045829514207149404}\n",
            "{'eval_loss': 3.7454216480255127, 'eval_runtime': 150.3141, 'eval_samples_per_second': 51.612, 'eval_steps_per_second': 6.453, 'epoch': 0.045829514207149404}\n",
            "{'loss': 4.0609, 'grad_norm': 1.0126186609268188, 'learning_rate': 4.919798350137489e-05, 'epoch': 0.04812098991750687}\n",
            "{'eval_loss': 3.7413556575775146, 'eval_runtime': 150.4153, 'eval_samples_per_second': 51.577, 'eval_steps_per_second': 6.449, 'epoch': 0.04812098991750687}\n",
            "{'loss': 4.1683, 'grad_norm': 0.9461212754249573, 'learning_rate': 4.915979223953559e-05, 'epoch': 0.05041246562786435}\n",
            "{'eval_loss': 3.737708568572998, 'eval_runtime': 150.2384, 'eval_samples_per_second': 51.638, 'eval_steps_per_second': 6.456, 'epoch': 0.05041246562786435}\n",
            "{'loss': 4.0783, 'grad_norm': 0.9588255286216736, 'learning_rate': 4.912160097769631e-05, 'epoch': 0.052703941338221816}\n",
            "{'eval_loss': 3.7354750633239746, 'eval_runtime': 150.4266, 'eval_samples_per_second': 51.573, 'eval_steps_per_second': 6.448, 'epoch': 0.052703941338221816}\n",
            "{'loss': 4.2873, 'grad_norm': 1.0622341632843018, 'learning_rate': 4.908340971585702e-05, 'epoch': 0.054995417048579284}\n",
            "{'eval_loss': 3.733085870742798, 'eval_runtime': 150.5151, 'eval_samples_per_second': 51.543, 'eval_steps_per_second': 6.445, 'epoch': 0.054995417048579284}\n",
            "{'loss': 4.0373, 'grad_norm': 1.0204976797103882, 'learning_rate': 4.904521845401772e-05, 'epoch': 0.05728689275893675}\n",
            "{'eval_loss': 3.7295119762420654, 'eval_runtime': 150.3506, 'eval_samples_per_second': 51.599, 'eval_steps_per_second': 6.452, 'epoch': 0.05728689275893675}\n",
            "{'loss': 4.128, 'grad_norm': 1.081589698791504, 'learning_rate': 4.9007027192178434e-05, 'epoch': 0.05957836846929423}\n",
            "{'eval_loss': 3.725872278213501, 'eval_runtime': 150.3962, 'eval_samples_per_second': 51.584, 'eval_steps_per_second': 6.45, 'epoch': 0.05957836846929423}\n",
            "{'loss': 4.0188, 'grad_norm': 0.9276064038276672, 'learning_rate': 4.896883593033914e-05, 'epoch': 0.061869844179651697}\n",
            "{'eval_loss': 3.7234110832214355, 'eval_runtime': 149.8579, 'eval_samples_per_second': 51.769, 'eval_steps_per_second': 6.473, 'epoch': 0.061869844179651697}\n",
            "{'loss': 3.9912, 'grad_norm': 1.1807403564453125, 'learning_rate': 4.8930644668499845e-05, 'epoch': 0.06416131989000917}\n",
            "{'eval_loss': 3.7196483612060547, 'eval_runtime': 149.7094, 'eval_samples_per_second': 51.82, 'eval_steps_per_second': 6.479, 'epoch': 0.06416131989000917}\n",
            "{'loss': 4.0649, 'grad_norm': 1.0068219900131226, 'learning_rate': 4.8892453406660557e-05, 'epoch': 0.06645279560036664}\n",
            "{'eval_loss': 3.717848777770996, 'eval_runtime': 149.7136, 'eval_samples_per_second': 51.819, 'eval_steps_per_second': 6.479, 'epoch': 0.06645279560036664}\n",
            "{'loss': 4.0178, 'grad_norm': 1.0410066843032837, 'learning_rate': 4.885426214482127e-05, 'epoch': 0.06874427131072411}\n",
            "{'eval_loss': 3.714733362197876, 'eval_runtime': 149.6496, 'eval_samples_per_second': 51.841, 'eval_steps_per_second': 6.482, 'epoch': 0.06874427131072411}\n",
            "{'loss': 3.9467, 'grad_norm': 1.151999592781067, 'learning_rate': 4.8816070882981974e-05, 'epoch': 0.07103574702108158}\n",
            "{'eval_loss': 3.712759017944336, 'eval_runtime': 149.7834, 'eval_samples_per_second': 51.795, 'eval_steps_per_second': 6.476, 'epoch': 0.07103574702108158}\n",
            "{'loss': 3.9609, 'grad_norm': 1.2252415418624878, 'learning_rate': 4.8777879621142686e-05, 'epoch': 0.07332722273143905}\n",
            "{'eval_loss': 3.7121334075927734, 'eval_runtime': 150.1334, 'eval_samples_per_second': 51.674, 'eval_steps_per_second': 6.461, 'epoch': 0.07332722273143905}\n",
            "{'loss': 4.0655, 'grad_norm': 1.0559462308883667, 'learning_rate': 4.873968835930339e-05, 'epoch': 0.07561869844179651}\n",
            "{'eval_loss': 3.710697889328003, 'eval_runtime': 150.1565, 'eval_samples_per_second': 51.666, 'eval_steps_per_second': 6.46, 'epoch': 0.07561869844179651}\n",
            "{'loss': 4.0628, 'grad_norm': 1.0819721221923828, 'learning_rate': 4.87014970974641e-05, 'epoch': 0.07791017415215398}\n",
            "{'eval_loss': 3.7088706493377686, 'eval_runtime': 150.1394, 'eval_samples_per_second': 51.672, 'eval_steps_per_second': 6.461, 'epoch': 0.07791017415215398}\n",
            "{'loss': 3.9093, 'grad_norm': 1.0680619478225708, 'learning_rate': 4.866330583562481e-05, 'epoch': 0.08020164986251145}\n",
            "{'eval_loss': 3.706191301345825, 'eval_runtime': 150.414, 'eval_samples_per_second': 51.578, 'eval_steps_per_second': 6.449, 'epoch': 0.08020164986251145}\n",
            "{'loss': 3.957, 'grad_norm': 0.924667239189148, 'learning_rate': 4.862511457378552e-05, 'epoch': 0.08249312557286893}\n",
            "{'eval_loss': 3.703944683074951, 'eval_runtime': 150.2499, 'eval_samples_per_second': 51.634, 'eval_steps_per_second': 6.456, 'epoch': 0.08249312557286893}\n",
            "{'loss': 4.0234, 'grad_norm': 1.1035925149917603, 'learning_rate': 4.858692331194623e-05, 'epoch': 0.0847846012832264}\n",
            "{'eval_loss': 3.7016844749450684, 'eval_runtime': 150.1747, 'eval_samples_per_second': 51.66, 'eval_steps_per_second': 6.459, 'epoch': 0.0847846012832264}\n",
            "{'loss': 4.2092, 'grad_norm': 1.0958224534988403, 'learning_rate': 4.854873205010694e-05, 'epoch': 0.08707607699358387}\n",
            "{'eval_loss': 3.700317859649658, 'eval_runtime': 149.9954, 'eval_samples_per_second': 51.722, 'eval_steps_per_second': 6.467, 'epoch': 0.08707607699358387}\n",
            "{'loss': 4.0207, 'grad_norm': 0.9797314405441284, 'learning_rate': 4.851054078826764e-05, 'epoch': 0.08936755270394134}\n",
            "{'eval_loss': 3.6989712715148926, 'eval_runtime': 150.1673, 'eval_samples_per_second': 51.662, 'eval_steps_per_second': 6.459, 'epoch': 0.08936755270394134}\n",
            "{'loss': 4.0417, 'grad_norm': 1.167202115058899, 'learning_rate': 4.8472349526428355e-05, 'epoch': 0.09165902841429881}\n",
            "{'eval_loss': 3.697390556335449, 'eval_runtime': 150.3847, 'eval_samples_per_second': 51.588, 'eval_steps_per_second': 6.45, 'epoch': 0.09165902841429881}\n",
            "{'loss': 3.8803, 'grad_norm': 1.1800416707992554, 'learning_rate': 4.843415826458906e-05, 'epoch': 0.09395050412465628}\n",
            "{'eval_loss': 3.696338176727295, 'eval_runtime': 150.331, 'eval_samples_per_second': 51.606, 'eval_steps_per_second': 6.452, 'epoch': 0.09395050412465628}\n",
            "{'loss': 3.8858, 'grad_norm': 0.9990566372871399, 'learning_rate': 4.839596700274978e-05, 'epoch': 0.09624197983501374}\n",
            "{'eval_loss': 3.6941192150115967, 'eval_runtime': 150.1616, 'eval_samples_per_second': 51.664, 'eval_steps_per_second': 6.46, 'epoch': 0.09624197983501374}\n",
            "{'loss': 3.9895, 'grad_norm': 0.8920756578445435, 'learning_rate': 4.8357775740910484e-05, 'epoch': 0.09853345554537121}\n",
            "{'eval_loss': 3.692706346511841, 'eval_runtime': 150.2689, 'eval_samples_per_second': 51.627, 'eval_steps_per_second': 6.455, 'epoch': 0.09853345554537121}\n",
            "{'loss': 4.0753, 'grad_norm': 1.3181594610214233, 'learning_rate': 4.831958447907119e-05, 'epoch': 0.1008249312557287}\n",
            "{'eval_loss': 3.6915149688720703, 'eval_runtime': 150.2136, 'eval_samples_per_second': 51.646, 'eval_steps_per_second': 6.457, 'epoch': 0.1008249312557287}\n",
            "{'loss': 3.9441, 'grad_norm': 0.9851364493370056, 'learning_rate': 4.82813932172319e-05, 'epoch': 0.10311640696608616}\n",
            "{'eval_loss': 3.690546989440918, 'eval_runtime': 150.2454, 'eval_samples_per_second': 51.636, 'eval_steps_per_second': 6.456, 'epoch': 0.10311640696608616}\n",
            "{'loss': 4.0806, 'grad_norm': 1.0105657577514648, 'learning_rate': 4.824320195539261e-05, 'epoch': 0.10540788267644363}\n",
            "{'eval_loss': 3.6895132064819336, 'eval_runtime': 150.2316, 'eval_samples_per_second': 51.64, 'eval_steps_per_second': 6.457, 'epoch': 0.10540788267644363}\n",
            "{'loss': 4.114, 'grad_norm': 1.0361809730529785, 'learning_rate': 4.820501069355331e-05, 'epoch': 0.1076993583868011}\n",
            "{'eval_loss': 3.6887130737304688, 'eval_runtime': 150.2329, 'eval_samples_per_second': 51.64, 'eval_steps_per_second': 6.457, 'epoch': 0.1076993583868011}\n",
            "{'loss': 4.0222, 'grad_norm': 0.909787654876709, 'learning_rate': 4.8166819431714024e-05, 'epoch': 0.10999083409715857}\n",
            "{'eval_loss': 3.686985969543457, 'eval_runtime': 150.321, 'eval_samples_per_second': 51.61, 'eval_steps_per_second': 6.453, 'epoch': 0.10999083409715857}\n",
            "{'loss': 3.9908, 'grad_norm': 1.1503241062164307, 'learning_rate': 4.8128628169874736e-05, 'epoch': 0.11228230980751604}\n",
            "{'eval_loss': 3.684011220932007, 'eval_runtime': 150.2271, 'eval_samples_per_second': 51.642, 'eval_steps_per_second': 6.457, 'epoch': 0.11228230980751604}\n",
            "{'loss': 4.0457, 'grad_norm': 1.2046235799789429, 'learning_rate': 4.809043690803544e-05, 'epoch': 0.1145737855178735}\n",
            "{'eval_loss': 3.682121992111206, 'eval_runtime': 150.4313, 'eval_samples_per_second': 51.572, 'eval_steps_per_second': 6.448, 'epoch': 0.1145737855178735}\n",
            "{'loss': 3.9521, 'grad_norm': 0.9681683778762817, 'learning_rate': 4.8052245646196154e-05, 'epoch': 0.11686526122823097}\n",
            "{'eval_loss': 3.681332588195801, 'eval_runtime': 150.3876, 'eval_samples_per_second': 51.587, 'eval_steps_per_second': 6.45, 'epoch': 0.11686526122823097}\n",
            "{'loss': 4.0339, 'grad_norm': 1.0135098695755005, 'learning_rate': 4.801405438435686e-05, 'epoch': 0.11915673693858846}\n",
            "{'eval_loss': 3.6809778213500977, 'eval_runtime': 150.1996, 'eval_samples_per_second': 51.651, 'eval_steps_per_second': 6.458, 'epoch': 0.11915673693858846}\n",
            "{'loss': 3.915, 'grad_norm': 1.0306134223937988, 'learning_rate': 4.797586312251757e-05, 'epoch': 0.12144821264894592}\n",
            "{'eval_loss': 3.6803088188171387, 'eval_runtime': 150.424, 'eval_samples_per_second': 51.574, 'eval_steps_per_second': 6.448, 'epoch': 0.12144821264894592}\n",
            "{'loss': 3.9977, 'grad_norm': 1.0405834913253784, 'learning_rate': 4.7937671860678276e-05, 'epoch': 0.12373968835930339}\n",
            "{'eval_loss': 3.679340124130249, 'eval_runtime': 150.2079, 'eval_samples_per_second': 51.648, 'eval_steps_per_second': 6.458, 'epoch': 0.12373968835930339}\n",
            "{'loss': 4.01, 'grad_norm': 0.9198901057243347, 'learning_rate': 4.789948059883899e-05, 'epoch': 0.12603116406966086}\n",
            "{'eval_loss': 3.6776981353759766, 'eval_runtime': 150.1152, 'eval_samples_per_second': 51.68, 'eval_steps_per_second': 6.462, 'epoch': 0.12603116406966086}\n",
            "{'loss': 4.0102, 'grad_norm': 1.1700128316879272, 'learning_rate': 4.78612893369997e-05, 'epoch': 0.12832263978001834}\n",
            "{'eval_loss': 3.67526912689209, 'eval_runtime': 150.0603, 'eval_samples_per_second': 51.699, 'eval_steps_per_second': 6.464, 'epoch': 0.12832263978001834}\n",
            "{'loss': 3.9757, 'grad_norm': 1.1971955299377441, 'learning_rate': 4.7823098075160405e-05, 'epoch': 0.1306141154903758}\n",
            "{'eval_loss': 3.6745331287384033, 'eval_runtime': 149.9078, 'eval_samples_per_second': 51.752, 'eval_steps_per_second': 6.471, 'epoch': 0.1306141154903758}\n",
            "{'loss': 4.0009, 'grad_norm': 1.1242035627365112, 'learning_rate': 4.778490681332111e-05, 'epoch': 0.13290559120073328}\n",
            "{'eval_loss': 3.673156261444092, 'eval_runtime': 150.0652, 'eval_samples_per_second': 51.698, 'eval_steps_per_second': 6.464, 'epoch': 0.13290559120073328}\n",
            "{'loss': 3.9962, 'grad_norm': 0.9984830021858215, 'learning_rate': 4.774671555148182e-05, 'epoch': 0.13519706691109074}\n",
            "{'eval_loss': 3.6716458797454834, 'eval_runtime': 150.5081, 'eval_samples_per_second': 51.545, 'eval_steps_per_second': 6.445, 'epoch': 0.13519706691109074}\n",
            "{'loss': 4.0422, 'grad_norm': 1.1071417331695557, 'learning_rate': 4.770852428964253e-05, 'epoch': 0.13748854262144822}\n",
            "{'eval_loss': 3.670776844024658, 'eval_runtime': 150.4972, 'eval_samples_per_second': 51.549, 'eval_steps_per_second': 6.445, 'epoch': 0.13748854262144822}\n",
            "{'loss': 4.0078, 'grad_norm': 1.0389866828918457, 'learning_rate': 4.767033302780324e-05, 'epoch': 0.13978001833180567}\n",
            "{'eval_loss': 3.669992446899414, 'eval_runtime': 150.4777, 'eval_samples_per_second': 51.556, 'eval_steps_per_second': 6.446, 'epoch': 0.13978001833180567}\n",
            "{'loss': 4.0552, 'grad_norm': 1.2471678256988525, 'learning_rate': 4.763214176596395e-05, 'epoch': 0.14207149404216315}\n",
            "{'eval_loss': 3.6692848205566406, 'eval_runtime': 150.4171, 'eval_samples_per_second': 51.577, 'eval_steps_per_second': 6.449, 'epoch': 0.14207149404216315}\n",
            "{'loss': 3.9018, 'grad_norm': 1.0725754499435425, 'learning_rate': 4.759395050412466e-05, 'epoch': 0.14436296975252064}\n",
            "{'eval_loss': 3.6690144538879395, 'eval_runtime': 150.4462, 'eval_samples_per_second': 51.567, 'eval_steps_per_second': 6.447, 'epoch': 0.14436296975252064}\n",
            "{'loss': 3.9487, 'grad_norm': 1.0756956338882446, 'learning_rate': 4.755575924228537e-05, 'epoch': 0.1466544454628781}\n",
            "{'eval_loss': 3.6682939529418945, 'eval_runtime': 150.4217, 'eval_samples_per_second': 51.575, 'eval_steps_per_second': 6.449, 'epoch': 0.1466544454628781}\n",
            "{'loss': 4.1622, 'grad_norm': 0.9978615641593933, 'learning_rate': 4.7517567980446075e-05, 'epoch': 0.14894592117323557}\n",
            "{'eval_loss': 3.667024850845337, 'eval_runtime': 150.4691, 'eval_samples_per_second': 51.559, 'eval_steps_per_second': 6.447, 'epoch': 0.14894592117323557}\n",
            "{'loss': 4.0715, 'grad_norm': 1.1674634218215942, 'learning_rate': 4.747937671860678e-05, 'epoch': 0.15123739688359303}\n",
            "{'eval_loss': 3.667295217514038, 'eval_runtime': 150.3968, 'eval_samples_per_second': 51.584, 'eval_steps_per_second': 6.45, 'epoch': 0.15123739688359303}\n",
            "{'loss': 3.9846, 'grad_norm': 0.9491991996765137, 'learning_rate': 4.744118545676749e-05, 'epoch': 0.1535288725939505}\n",
            "{'eval_loss': 3.666682243347168, 'eval_runtime': 150.3395, 'eval_samples_per_second': 51.603, 'eval_steps_per_second': 6.452, 'epoch': 0.1535288725939505}\n",
            "{'loss': 3.9711, 'grad_norm': 0.9631597995758057, 'learning_rate': 4.7402994194928204e-05, 'epoch': 0.15582034830430797}\n",
            "{'eval_loss': 3.665921926498413, 'eval_runtime': 150.2259, 'eval_samples_per_second': 51.642, 'eval_steps_per_second': 6.457, 'epoch': 0.15582034830430797}\n",
            "{'loss': 3.9678, 'grad_norm': 1.1400398015975952, 'learning_rate': 4.7364802933088916e-05, 'epoch': 0.15811182401466545}\n",
            "{'eval_loss': 3.6633307933807373, 'eval_runtime': 150.3726, 'eval_samples_per_second': 51.592, 'eval_steps_per_second': 6.451, 'epoch': 0.15811182401466545}\n",
            "{'loss': 3.9265, 'grad_norm': 0.9392791390419006, 'learning_rate': 4.732661167124962e-05, 'epoch': 0.1604032997250229}\n",
            "{'eval_loss': 3.6609883308410645, 'eval_runtime': 150.2167, 'eval_samples_per_second': 51.645, 'eval_steps_per_second': 6.457, 'epoch': 0.1604032997250229}\n",
            "{'loss': 3.8865, 'grad_norm': 0.8886269927024841, 'learning_rate': 4.7288420409410327e-05, 'epoch': 0.16269477543538038}\n",
            "{'eval_loss': 3.659695863723755, 'eval_runtime': 150.1446, 'eval_samples_per_second': 51.67, 'eval_steps_per_second': 6.46, 'epoch': 0.16269477543538038}\n",
            "{'loss': 3.8891, 'grad_norm': 1.0371094942092896, 'learning_rate': 4.725022914757104e-05, 'epoch': 0.16498625114573787}\n",
            "{'eval_loss': 3.658679723739624, 'eval_runtime': 150.2043, 'eval_samples_per_second': 51.65, 'eval_steps_per_second': 6.458, 'epoch': 0.16498625114573787}\n",
            "{'loss': 3.9397, 'grad_norm': 1.070608377456665, 'learning_rate': 4.7212037885731744e-05, 'epoch': 0.16727772685609532}\n",
            "{'eval_loss': 3.6583242416381836, 'eval_runtime': 150.2319, 'eval_samples_per_second': 51.64, 'eval_steps_per_second': 6.457, 'epoch': 0.16727772685609532}\n",
            "{'loss': 3.9791, 'grad_norm': 1.062888503074646, 'learning_rate': 4.7173846623892456e-05, 'epoch': 0.1695692025664528}\n",
            "{'eval_loss': 3.658442497253418, 'eval_runtime': 150.0852, 'eval_samples_per_second': 51.691, 'eval_steps_per_second': 6.463, 'epoch': 0.1695692025664528}\n",
            "{'loss': 3.9255, 'grad_norm': 1.0231478214263916, 'learning_rate': 4.713565536205317e-05, 'epoch': 0.17186067827681026}\n",
            "{'eval_loss': 3.6582071781158447, 'eval_runtime': 150.03, 'eval_samples_per_second': 51.71, 'eval_steps_per_second': 6.465, 'epoch': 0.17186067827681026}\n",
            "{'loss': 3.9713, 'grad_norm': 0.9764264225959778, 'learning_rate': 4.709746410021387e-05, 'epoch': 0.17415215398716774}\n",
            "{'eval_loss': 3.6579413414001465, 'eval_runtime': 150.0682, 'eval_samples_per_second': 51.696, 'eval_steps_per_second': 6.464, 'epoch': 0.17415215398716774}\n",
            "{'loss': 3.9387, 'grad_norm': 1.0371960401535034, 'learning_rate': 4.705927283837458e-05, 'epoch': 0.1764436296975252}\n",
            "{'eval_loss': 3.6565003395080566, 'eval_runtime': 149.9887, 'eval_samples_per_second': 51.724, 'eval_steps_per_second': 6.467, 'epoch': 0.1764436296975252}\n",
            "{'loss': 3.9294, 'grad_norm': 1.0229599475860596, 'learning_rate': 4.702108157653529e-05, 'epoch': 0.17873510540788268}\n",
            "{'eval_loss': 3.6546759605407715, 'eval_runtime': 150.0718, 'eval_samples_per_second': 51.695, 'eval_steps_per_second': 6.464, 'epoch': 0.17873510540788268}\n",
            "{'loss': 3.9503, 'grad_norm': 0.96379554271698, 'learning_rate': 4.6982890314695996e-05, 'epoch': 0.18102658111824016}\n",
            "{'eval_loss': 3.654282808303833, 'eval_runtime': 150.3009, 'eval_samples_per_second': 51.616, 'eval_steps_per_second': 6.454, 'epoch': 0.18102658111824016}\n",
            "{'loss': 3.9294, 'grad_norm': 1.1010611057281494, 'learning_rate': 4.694469905285671e-05, 'epoch': 0.18331805682859761}\n",
            "{'eval_loss': 3.6532318592071533, 'eval_runtime': 150.1738, 'eval_samples_per_second': 51.66, 'eval_steps_per_second': 6.459, 'epoch': 0.18331805682859761}\n",
            "{'loss': 3.941, 'grad_norm': 0.9865167140960693, 'learning_rate': 4.690650779101742e-05, 'epoch': 0.1856095325389551}\n",
            "{'eval_loss': 3.6521410942077637, 'eval_runtime': 150.2407, 'eval_samples_per_second': 51.637, 'eval_steps_per_second': 6.456, 'epoch': 0.1856095325389551}\n",
            "{'loss': 4.0571, 'grad_norm': 1.0082590579986572, 'learning_rate': 4.6868316529178125e-05, 'epoch': 0.18790100824931255}\n",
            "{'eval_loss': 3.651855945587158, 'eval_runtime': 150.2988, 'eval_samples_per_second': 51.617, 'eval_steps_per_second': 6.454, 'epoch': 0.18790100824931255}\n",
            "{'loss': 4.0044, 'grad_norm': 0.8748037219047546, 'learning_rate': 4.683012526733884e-05, 'epoch': 0.19019248395967003}\n",
            "{'eval_loss': 3.6516528129577637, 'eval_runtime': 150.3984, 'eval_samples_per_second': 51.583, 'eval_steps_per_second': 6.45, 'epoch': 0.19019248395967003}\n",
            "{'loss': 3.957, 'grad_norm': 0.9798491597175598, 'learning_rate': 4.679193400549954e-05, 'epoch': 0.1924839596700275}\n",
            "{'eval_loss': 3.6509652137756348, 'eval_runtime': 150.4131, 'eval_samples_per_second': 51.578, 'eval_steps_per_second': 6.449, 'epoch': 0.1924839596700275}\n",
            "{'loss': 3.8814, 'grad_norm': 1.1403961181640625, 'learning_rate': 4.675374274366025e-05, 'epoch': 0.19477543538038497}\n",
            "{'eval_loss': 3.6500039100646973, 'eval_runtime': 150.3034, 'eval_samples_per_second': 51.616, 'eval_steps_per_second': 6.454, 'epoch': 0.19477543538038497}\n",
            "{'loss': 3.922, 'grad_norm': 1.0785101652145386, 'learning_rate': 4.671555148182096e-05, 'epoch': 0.19706691109074242}\n",
            "{'eval_loss': 3.64851450920105, 'eval_runtime': 150.2745, 'eval_samples_per_second': 51.626, 'eval_steps_per_second': 6.455, 'epoch': 0.19706691109074242}\n",
            "{'loss': 3.9736, 'grad_norm': 1.1146854162216187, 'learning_rate': 4.667736021998167e-05, 'epoch': 0.1993583868010999}\n",
            "{'eval_loss': 3.6470789909362793, 'eval_runtime': 150.3735, 'eval_samples_per_second': 51.592, 'eval_steps_per_second': 6.451, 'epoch': 0.1993583868010999}\n",
            "{'loss': 3.8646, 'grad_norm': 1.0979350805282593, 'learning_rate': 4.6639168958142384e-05, 'epoch': 0.2016498625114574}\n",
            "{'eval_loss': 3.6462082862854004, 'eval_runtime': 150.3608, 'eval_samples_per_second': 51.596, 'eval_steps_per_second': 6.451, 'epoch': 0.2016498625114574}\n",
            "{'loss': 3.9325, 'grad_norm': 1.1981555223464966, 'learning_rate': 4.660097769630309e-05, 'epoch': 0.20394133822181484}\n",
            "{'eval_loss': 3.645470380783081, 'eval_runtime': 150.3734, 'eval_samples_per_second': 51.592, 'eval_steps_per_second': 6.451, 'epoch': 0.20394133822181484}\n",
            "{'loss': 3.999, 'grad_norm': 0.9413831233978271, 'learning_rate': 4.6562786434463794e-05, 'epoch': 0.20623281393217233}\n",
            "{'eval_loss': 3.6446526050567627, 'eval_runtime': 150.3766, 'eval_samples_per_second': 51.59, 'eval_steps_per_second': 6.45, 'epoch': 0.20623281393217233}\n",
            "{'loss': 3.8707, 'grad_norm': 0.9767171144485474, 'learning_rate': 4.6524595172624506e-05, 'epoch': 0.20852428964252978}\n",
            "{'eval_loss': 3.644158124923706, 'eval_runtime': 150.3028, 'eval_samples_per_second': 51.616, 'eval_steps_per_second': 6.454, 'epoch': 0.20852428964252978}\n",
            "{'loss': 3.9739, 'grad_norm': 1.2653493881225586, 'learning_rate': 4.648640391078521e-05, 'epoch': 0.21081576535288726}\n",
            "{'eval_loss': 3.643768310546875, 'eval_runtime': 150.3232, 'eval_samples_per_second': 51.609, 'eval_steps_per_second': 6.453, 'epoch': 0.21081576535288726}\n",
            "{'loss': 4.1141, 'grad_norm': 0.8792517781257629, 'learning_rate': 4.6448212648945924e-05, 'epoch': 0.21310724106324472}\n",
            "{'eval_loss': 3.642962694168091, 'eval_runtime': 150.3135, 'eval_samples_per_second': 51.612, 'eval_steps_per_second': 6.453, 'epoch': 0.21310724106324472}\n",
            "{'loss': 3.9587, 'grad_norm': 0.8750298619270325, 'learning_rate': 4.6410021387106636e-05, 'epoch': 0.2153987167736022}\n",
            "{'eval_loss': 3.642378568649292, 'eval_runtime': 150.3825, 'eval_samples_per_second': 51.588, 'eval_steps_per_second': 6.45, 'epoch': 0.2153987167736022}\n",
            "{'loss': 3.9815, 'grad_norm': 1.024017572402954, 'learning_rate': 4.637183012526734e-05, 'epoch': 0.21769019248395968}\n",
            "{'eval_loss': 3.6414954662323, 'eval_runtime': 150.2623, 'eval_samples_per_second': 51.63, 'eval_steps_per_second': 6.455, 'epoch': 0.21769019248395968}\n",
            "{'loss': 3.9952, 'grad_norm': 1.1794449090957642, 'learning_rate': 4.6333638863428046e-05, 'epoch': 0.21998166819431714}\n",
            "{'eval_loss': 3.641282558441162, 'eval_runtime': 150.288, 'eval_samples_per_second': 51.621, 'eval_steps_per_second': 6.454, 'epoch': 0.21998166819431714}\n",
            "{'loss': 3.8781, 'grad_norm': 0.8704323768615723, 'learning_rate': 4.629544760158876e-05, 'epoch': 0.22227314390467462}\n",
            "{'eval_loss': 3.6413707733154297, 'eval_runtime': 150.2494, 'eval_samples_per_second': 51.634, 'eval_steps_per_second': 6.456, 'epoch': 0.22227314390467462}\n",
            "{'loss': 3.9222, 'grad_norm': 1.0120760202407837, 'learning_rate': 4.6257256339749463e-05, 'epoch': 0.22456461961503207}\n",
            "{'eval_loss': 3.640347480773926, 'eval_runtime': 150.2898, 'eval_samples_per_second': 51.62, 'eval_steps_per_second': 6.454, 'epoch': 0.22456461961503207}\n",
            "{'loss': 3.9796, 'grad_norm': 0.9753338098526001, 'learning_rate': 4.6219065077910175e-05, 'epoch': 0.22685609532538956}\n",
            "{'eval_loss': 3.6398842334747314, 'eval_runtime': 150.1784, 'eval_samples_per_second': 51.659, 'eval_steps_per_second': 6.459, 'epoch': 0.22685609532538956}\n",
            "{'loss': 3.9537, 'grad_norm': 0.9472567439079285, 'learning_rate': 4.618087381607089e-05, 'epoch': 0.229147571035747}\n",
            "{'eval_loss': 3.6399166584014893, 'eval_runtime': 150.1239, 'eval_samples_per_second': 51.677, 'eval_steps_per_second': 6.461, 'epoch': 0.229147571035747}\n",
            "{'loss': 3.9887, 'grad_norm': 0.9255644083023071, 'learning_rate': 4.614268255423159e-05, 'epoch': 0.2314390467461045}\n",
            "{'eval_loss': 3.6394100189208984, 'eval_runtime': 150.1032, 'eval_samples_per_second': 51.684, 'eval_steps_per_second': 6.462, 'epoch': 0.2314390467461045}\n",
            "{'loss': 3.923, 'grad_norm': 1.0996681451797485, 'learning_rate': 4.6104491292392305e-05, 'epoch': 0.23373052245646195}\n",
            "{'eval_loss': 3.6380152702331543, 'eval_runtime': 149.9137, 'eval_samples_per_second': 51.75, 'eval_steps_per_second': 6.47, 'epoch': 0.23373052245646195}\n",
            "{'loss': 3.9074, 'grad_norm': 1.1003557443618774, 'learning_rate': 4.606630003055301e-05, 'epoch': 0.23602199816681943}\n",
            "{'eval_loss': 3.6375389099121094, 'eval_runtime': 149.8205, 'eval_samples_per_second': 51.782, 'eval_steps_per_second': 6.474, 'epoch': 0.23602199816681943}\n",
            "{'loss': 3.9098, 'grad_norm': 0.9558295011520386, 'learning_rate': 4.6028108768713715e-05, 'epoch': 0.2383134738771769}\n",
            "{'eval_loss': 3.636538028717041, 'eval_runtime': 149.78, 'eval_samples_per_second': 51.796, 'eval_steps_per_second': 6.476, 'epoch': 0.2383134738771769}\n",
            "{'loss': 3.944, 'grad_norm': 1.0051625967025757, 'learning_rate': 4.598991750687443e-05, 'epoch': 0.24060494958753437}\n",
            "{'eval_loss': 3.635261058807373, 'eval_runtime': 149.7493, 'eval_samples_per_second': 51.807, 'eval_steps_per_second': 6.477, 'epoch': 0.24060494958753437}\n",
            "{'loss': 4.0358, 'grad_norm': 0.9891473054885864, 'learning_rate': 4.595172624503514e-05, 'epoch': 0.24289642529789185}\n",
            "{'eval_loss': 3.6344056129455566, 'eval_runtime': 149.7646, 'eval_samples_per_second': 51.801, 'eval_steps_per_second': 6.477, 'epoch': 0.24289642529789185}\n",
            "{'loss': 3.9096, 'grad_norm': 0.9006747603416443, 'learning_rate': 4.591353498319585e-05, 'epoch': 0.2451879010082493}\n",
            "{'eval_loss': 3.6336231231689453, 'eval_runtime': 149.6683, 'eval_samples_per_second': 51.835, 'eval_steps_per_second': 6.481, 'epoch': 0.2451879010082493}\n",
            "{'loss': 4.0058, 'grad_norm': 1.070621371269226, 'learning_rate': 4.587534372135656e-05, 'epoch': 0.24747937671860679}\n",
            "{'eval_loss': 3.632499933242798, 'eval_runtime': 149.7741, 'eval_samples_per_second': 51.798, 'eval_steps_per_second': 6.476, 'epoch': 0.24747937671860679}\n",
            "{'loss': 4.0064, 'grad_norm': 0.9427714347839355, 'learning_rate': 4.583715245951726e-05, 'epoch': 0.24977085242896424}\n",
            "{'eval_loss': 3.6322271823883057, 'eval_runtime': 150.0485, 'eval_samples_per_second': 51.703, 'eval_steps_per_second': 6.465, 'epoch': 0.24977085242896424}\n",
            "{'loss': 4.0236, 'grad_norm': 1.032687783241272, 'learning_rate': 4.5798961197677974e-05, 'epoch': 0.2520623281393217}\n",
            "{'eval_loss': 3.6315863132476807, 'eval_runtime': 149.9438, 'eval_samples_per_second': 51.739, 'eval_steps_per_second': 6.469, 'epoch': 0.2520623281393217}\n",
            "{'loss': 3.9636, 'grad_norm': 1.081075668334961, 'learning_rate': 4.576076993583868e-05, 'epoch': 0.2543538038496792}\n",
            "{'eval_loss': 3.630776882171631, 'eval_runtime': 149.9541, 'eval_samples_per_second': 51.736, 'eval_steps_per_second': 6.469, 'epoch': 0.2543538038496792}\n",
            "{'loss': 3.9479, 'grad_norm': 1.0900336503982544, 'learning_rate': 4.572257867399939e-05, 'epoch': 0.2566452795600367}\n",
            "{'eval_loss': 3.630789279937744, 'eval_runtime': 149.9343, 'eval_samples_per_second': 51.743, 'eval_steps_per_second': 6.47, 'epoch': 0.2566452795600367}\n",
            "{'loss': 3.8986, 'grad_norm': 1.016157865524292, 'learning_rate': 4.56843874121601e-05, 'epoch': 0.2589367552703941}\n",
            "{'eval_loss': 3.630836248397827, 'eval_runtime': 149.8597, 'eval_samples_per_second': 51.768, 'eval_steps_per_second': 6.473, 'epoch': 0.2589367552703941}\n",
            "{'loss': 3.8563, 'grad_norm': 1.037369728088379, 'learning_rate': 4.564619615032081e-05, 'epoch': 0.2612282309807516}\n",
            "{'eval_loss': 3.63078236579895, 'eval_runtime': 149.9317, 'eval_samples_per_second': 51.744, 'eval_steps_per_second': 6.47, 'epoch': 0.2612282309807516}\n",
            "{'loss': 4.0274, 'grad_norm': 0.9999097585678101, 'learning_rate': 4.560800488848152e-05, 'epoch': 0.2635197066911091}\n",
            "{'eval_loss': 3.63055419921875, 'eval_runtime': 149.9299, 'eval_samples_per_second': 51.744, 'eval_steps_per_second': 6.47, 'epoch': 0.2635197066911091}\n",
            "{'loss': 3.9245, 'grad_norm': 1.1270301342010498, 'learning_rate': 4.5569813626642226e-05, 'epoch': 0.26581118240146656}\n",
            "{'eval_loss': 3.6301231384277344, 'eval_runtime': 150.0691, 'eval_samples_per_second': 51.696, 'eval_steps_per_second': 6.464, 'epoch': 0.26581118240146656}\n",
            "{'loss': 3.9883, 'grad_norm': 0.8416671752929688, 'learning_rate': 4.553162236480293e-05, 'epoch': 0.268102658111824}\n",
            "{'eval_loss': 3.629788637161255, 'eval_runtime': 150.1576, 'eval_samples_per_second': 51.666, 'eval_steps_per_second': 6.46, 'epoch': 0.268102658111824}\n",
            "{'loss': 3.9847, 'grad_norm': 1.0767277479171753, 'learning_rate': 4.549343110296364e-05, 'epoch': 0.27039413382218147}\n",
            "{'eval_loss': 3.628892660140991, 'eval_runtime': 150.2038, 'eval_samples_per_second': 51.65, 'eval_steps_per_second': 6.458, 'epoch': 0.27039413382218147}\n",
            "{'loss': 3.9441, 'grad_norm': 0.9770700931549072, 'learning_rate': 4.5455239841124355e-05, 'epoch': 0.27268560953253895}\n",
            "{'eval_loss': 3.628325939178467, 'eval_runtime': 150.0927, 'eval_samples_per_second': 51.688, 'eval_steps_per_second': 6.463, 'epoch': 0.27268560953253895}\n",
            "{'loss': 3.9989, 'grad_norm': 1.0028935670852661, 'learning_rate': 4.541704857928506e-05, 'epoch': 0.27497708524289644}\n",
            "{'eval_loss': 3.6275203227996826, 'eval_runtime': 150.0279, 'eval_samples_per_second': 51.71, 'eval_steps_per_second': 6.465, 'epoch': 0.27497708524289644}\n",
            "{'loss': 3.929, 'grad_norm': 0.9735599160194397, 'learning_rate': 4.537885731744577e-05, 'epoch': 0.2772685609532539}\n",
            "{'eval_loss': 3.6273910999298096, 'eval_runtime': 149.98, 'eval_samples_per_second': 51.727, 'eval_steps_per_second': 6.468, 'epoch': 0.2772685609532539}\n",
            "{'loss': 4.0235, 'grad_norm': 0.9301197528839111, 'learning_rate': 4.534066605560648e-05, 'epoch': 0.27956003666361134}\n",
            "{'eval_loss': 3.6275978088378906, 'eval_runtime': 149.7655, 'eval_samples_per_second': 51.801, 'eval_steps_per_second': 6.477, 'epoch': 0.27956003666361134}\n",
            "{'loss': 3.9777, 'grad_norm': 1.0228242874145508, 'learning_rate': 4.530247479376718e-05, 'epoch': 0.2818515123739688}\n",
            "{'eval_loss': 3.627272129058838, 'eval_runtime': 149.6933, 'eval_samples_per_second': 51.826, 'eval_steps_per_second': 6.48, 'epoch': 0.2818515123739688}\n",
            "{'loss': 3.8697, 'grad_norm': 1.0217742919921875, 'learning_rate': 4.5264283531927895e-05, 'epoch': 0.2841429880843263}\n",
            "{'eval_loss': 3.626497507095337, 'eval_runtime': 149.5855, 'eval_samples_per_second': 51.863, 'eval_steps_per_second': 6.485, 'epoch': 0.2841429880843263}\n",
            "{'loss': 3.9338, 'grad_norm': 0.9754296541213989, 'learning_rate': 4.522609227008861e-05, 'epoch': 0.2864344637946838}\n",
            "{'eval_loss': 3.62550950050354, 'eval_runtime': 149.587, 'eval_samples_per_second': 51.863, 'eval_steps_per_second': 6.485, 'epoch': 0.2864344637946838}\n",
            "{'loss': 3.8345, 'grad_norm': 0.9727302193641663, 'learning_rate': 4.518790100824932e-05, 'epoch': 0.2887259395050413}\n",
            "{'eval_loss': 3.624234199523926, 'eval_runtime': 149.5624, 'eval_samples_per_second': 51.871, 'eval_steps_per_second': 6.486, 'epoch': 0.2887259395050413}\n",
            "{'loss': 3.9395, 'grad_norm': 0.908344566822052, 'learning_rate': 4.5149709746410024e-05, 'epoch': 0.2910174152153987}\n",
            "{'eval_loss': 3.6236913204193115, 'eval_runtime': 149.6071, 'eval_samples_per_second': 51.856, 'eval_steps_per_second': 6.484, 'epoch': 0.2910174152153987}\n",
            "{'loss': 3.8742, 'grad_norm': 1.0176799297332764, 'learning_rate': 4.511151848457073e-05, 'epoch': 0.2933088909257562}\n",
            "{'eval_loss': 3.623624563217163, 'eval_runtime': 149.5226, 'eval_samples_per_second': 51.885, 'eval_steps_per_second': 6.487, 'epoch': 0.2933088909257562}\n",
            "{'loss': 3.8869, 'grad_norm': 1.0800364017486572, 'learning_rate': 4.507332722273144e-05, 'epoch': 0.29560036663611367}\n",
            "{'eval_loss': 3.6228344440460205, 'eval_runtime': 149.5526, 'eval_samples_per_second': 51.875, 'eval_steps_per_second': 6.486, 'epoch': 0.29560036663611367}\n",
            "{'loss': 3.9567, 'grad_norm': 1.0179444551467896, 'learning_rate': 4.503513596089215e-05, 'epoch': 0.29789184234647115}\n",
            "{'eval_loss': 3.6228654384613037, 'eval_runtime': 149.5253, 'eval_samples_per_second': 51.884, 'eval_steps_per_second': 6.487, 'epoch': 0.29789184234647115}\n",
            "{'loss': 3.9188, 'grad_norm': 0.9899793267250061, 'learning_rate': 4.499694469905286e-05, 'epoch': 0.3001833180568286}\n",
            "{'eval_loss': 3.622844934463501, 'eval_runtime': 149.6527, 'eval_samples_per_second': 51.84, 'eval_steps_per_second': 6.482, 'epoch': 0.3001833180568286}\n",
            "{'loss': 3.8632, 'grad_norm': 0.9897602200508118, 'learning_rate': 4.495875343721357e-05, 'epoch': 0.30247479376718606}\n",
            "{'eval_loss': 3.6226842403411865, 'eval_runtime': 149.5653, 'eval_samples_per_second': 51.87, 'eval_steps_per_second': 6.485, 'epoch': 0.30247479376718606}\n",
            "{'loss': 3.8921, 'grad_norm': 1.0397595167160034, 'learning_rate': 4.4920562175374276e-05, 'epoch': 0.30476626947754354}\n",
            "{'eval_loss': 3.6218526363372803, 'eval_runtime': 149.6048, 'eval_samples_per_second': 51.857, 'eval_steps_per_second': 6.484, 'epoch': 0.30476626947754354}\n",
            "{'loss': 3.8991, 'grad_norm': 1.1396205425262451, 'learning_rate': 4.488237091353499e-05, 'epoch': 0.307057745187901}\n",
            "{'eval_loss': 3.6219213008880615, 'eval_runtime': 149.5489, 'eval_samples_per_second': 51.876, 'eval_steps_per_second': 6.486, 'epoch': 0.307057745187901}\n",
            "{'loss': 3.8766, 'grad_norm': 0.9108911752700806, 'learning_rate': 4.4844179651695694e-05, 'epoch': 0.3093492208982585}\n",
            "{'eval_loss': 3.6219980716705322, 'eval_runtime': 149.6416, 'eval_samples_per_second': 51.844, 'eval_steps_per_second': 6.482, 'epoch': 0.3093492208982585}\n",
            "{'loss': 3.9454, 'grad_norm': 1.1146166324615479, 'learning_rate': 4.48059883898564e-05, 'epoch': 0.31164069660861593}\n",
            "{'eval_loss': 3.6216177940368652, 'eval_runtime': 149.5336, 'eval_samples_per_second': 51.881, 'eval_steps_per_second': 6.487, 'epoch': 0.31164069660861593}\n",
            "{'loss': 3.9266, 'grad_norm': 1.0081104040145874, 'learning_rate': 4.476779712801711e-05, 'epoch': 0.3139321723189734}\n",
            "{'eval_loss': 3.620961904525757, 'eval_runtime': 149.5682, 'eval_samples_per_second': 51.869, 'eval_steps_per_second': 6.485, 'epoch': 0.3139321723189734}\n",
            "{'loss': 3.8883, 'grad_norm': 1.0907081365585327, 'learning_rate': 4.472960586617782e-05, 'epoch': 0.3162236480293309}\n",
            "{'eval_loss': 3.620859146118164, 'eval_runtime': 149.6484, 'eval_samples_per_second': 51.842, 'eval_steps_per_second': 6.482, 'epoch': 0.3162236480293309}\n",
            "{'loss': 3.931, 'grad_norm': 0.9781312346458435, 'learning_rate': 4.469141460433853e-05, 'epoch': 0.3185151237396884}\n",
            "{'eval_loss': 3.620509147644043, 'eval_runtime': 149.6289, 'eval_samples_per_second': 51.848, 'eval_steps_per_second': 6.483, 'epoch': 0.3185151237396884}\n",
            "{'loss': 3.863, 'grad_norm': 1.0451884269714355, 'learning_rate': 4.465322334249924e-05, 'epoch': 0.3208065994500458}\n",
            "{'eval_loss': 3.6198949813842773, 'eval_runtime': 149.5127, 'eval_samples_per_second': 51.889, 'eval_steps_per_second': 6.488, 'epoch': 0.3208065994500458}\n",
            "{'loss': 3.898, 'grad_norm': 1.1031454801559448, 'learning_rate': 4.4615032080659945e-05, 'epoch': 0.3230980751604033}\n",
            "{'eval_loss': 3.618623971939087, 'eval_runtime': 149.6354, 'eval_samples_per_second': 51.846, 'eval_steps_per_second': 6.482, 'epoch': 0.3230980751604033}\n",
            "{'loss': 3.8641, 'grad_norm': 0.9845146536827087, 'learning_rate': 4.457684081882065e-05, 'epoch': 0.32538955087076077}\n",
            "{'eval_loss': 3.6174910068511963, 'eval_runtime': 149.6331, 'eval_samples_per_second': 51.847, 'eval_steps_per_second': 6.483, 'epoch': 0.32538955087076077}\n",
            "{'loss': 3.9594, 'grad_norm': 1.541006326675415, 'learning_rate': 4.453864955698136e-05, 'epoch': 0.32768102658111825}\n",
            "{'eval_loss': 3.617082357406616, 'eval_runtime': 149.5095, 'eval_samples_per_second': 51.89, 'eval_steps_per_second': 6.488, 'epoch': 0.32768102658111825}\n",
            "{'loss': 3.9031, 'grad_norm': 1.0737130641937256, 'learning_rate': 4.4500458295142075e-05, 'epoch': 0.32997250229147573}\n",
            "{'eval_loss': 3.616567850112915, 'eval_runtime': 149.565, 'eval_samples_per_second': 51.87, 'eval_steps_per_second': 6.485, 'epoch': 0.32997250229147573}\n",
            "{'loss': 3.9338, 'grad_norm': 1.2715907096862793, 'learning_rate': 4.446226703330279e-05, 'epoch': 0.33226397800183316}\n",
            "{'eval_loss': 3.6157727241516113, 'eval_runtime': 149.5637, 'eval_samples_per_second': 51.871, 'eval_steps_per_second': 6.486, 'epoch': 0.33226397800183316}\n",
            "{'loss': 3.8592, 'grad_norm': 1.14248526096344, 'learning_rate': 4.442407577146349e-05, 'epoch': 0.33455545371219064}\n",
            "{'eval_loss': 3.6148312091827393, 'eval_runtime': 149.4449, 'eval_samples_per_second': 51.912, 'eval_steps_per_second': 6.491, 'epoch': 0.33455545371219064}\n",
            "{'loss': 3.8827, 'grad_norm': 1.0978337526321411, 'learning_rate': 4.43858845096242e-05, 'epoch': 0.3368469294225481}\n",
            "{'eval_loss': 3.6145758628845215, 'eval_runtime': 149.6381, 'eval_samples_per_second': 51.845, 'eval_steps_per_second': 6.482, 'epoch': 0.3368469294225481}\n",
            "{'loss': 4.0312, 'grad_norm': 1.1666597127914429, 'learning_rate': 4.434769324778491e-05, 'epoch': 0.3391384051329056}\n",
            "{'eval_loss': 3.6144936084747314, 'eval_runtime': 149.5692, 'eval_samples_per_second': 51.869, 'eval_steps_per_second': 6.485, 'epoch': 0.3391384051329056}\n",
            "{'loss': 4.0223, 'grad_norm': 1.0043584108352661, 'learning_rate': 4.4309501985945615e-05, 'epoch': 0.34142988084326303}\n",
            "{'eval_loss': 3.614311933517456, 'eval_runtime': 149.6208, 'eval_samples_per_second': 51.851, 'eval_steps_per_second': 6.483, 'epoch': 0.34142988084326303}\n",
            "{'loss': 3.9282, 'grad_norm': 0.9986211657524109, 'learning_rate': 4.427131072410633e-05, 'epoch': 0.3437213565536205}\n",
            "{'eval_loss': 3.6145482063293457, 'eval_runtime': 149.4591, 'eval_samples_per_second': 51.907, 'eval_steps_per_second': 6.49, 'epoch': 0.3437213565536205}\n",
            "{'loss': 3.8496, 'grad_norm': 1.0048478841781616, 'learning_rate': 4.423311946226704e-05, 'epoch': 0.346012832263978}\n",
            "{'eval_loss': 3.613992214202881, 'eval_runtime': 149.5985, 'eval_samples_per_second': 51.859, 'eval_steps_per_second': 6.484, 'epoch': 0.346012832263978}\n",
            "{'loss': 3.9457, 'grad_norm': 1.3194464445114136, 'learning_rate': 4.4194928200427744e-05, 'epoch': 0.3483043079743355}\n",
            "{'eval_loss': 3.613682985305786, 'eval_runtime': 149.6433, 'eval_samples_per_second': 51.843, 'eval_steps_per_second': 6.482, 'epoch': 0.3483043079743355}\n",
            "{'loss': 3.9029, 'grad_norm': 0.9784181118011475, 'learning_rate': 4.4156736938588456e-05, 'epoch': 0.35059578368469296}\n",
            "{'eval_loss': 3.613455057144165, 'eval_runtime': 149.4668, 'eval_samples_per_second': 51.905, 'eval_steps_per_second': 6.49, 'epoch': 0.35059578368469296}\n",
            "{'loss': 3.9517, 'grad_norm': 0.9649434089660645, 'learning_rate': 4.411854567674916e-05, 'epoch': 0.3528872593950504}\n",
            "{'eval_loss': 3.612751007080078, 'eval_runtime': 149.5222, 'eval_samples_per_second': 51.885, 'eval_steps_per_second': 6.487, 'epoch': 0.3528872593950504}\n",
            "{'loss': 3.9082, 'grad_norm': 1.0262515544891357, 'learning_rate': 4.4080354414909867e-05, 'epoch': 0.3551787351054079}\n",
            "{'eval_loss': 3.6118712425231934, 'eval_runtime': 149.4907, 'eval_samples_per_second': 51.896, 'eval_steps_per_second': 6.489, 'epoch': 0.3551787351054079}\n",
            "{'loss': 3.8523, 'grad_norm': 0.937338650226593, 'learning_rate': 4.404216315307058e-05, 'epoch': 0.35747021081576535}\n",
            "{'eval_loss': 3.6112983226776123, 'eval_runtime': 149.5779, 'eval_samples_per_second': 51.866, 'eval_steps_per_second': 6.485, 'epoch': 0.35747021081576535}\n",
            "{'loss': 3.9873, 'grad_norm': 1.0106569528579712, 'learning_rate': 4.400397189123129e-05, 'epoch': 0.35976168652612284}\n",
            "{'eval_loss': 3.6111791133880615, 'eval_runtime': 149.4884, 'eval_samples_per_second': 51.897, 'eval_steps_per_second': 6.489, 'epoch': 0.35976168652612284}\n",
            "{'loss': 3.9603, 'grad_norm': 1.1134512424468994, 'learning_rate': 4.3965780629391996e-05, 'epoch': 0.3620531622364803}\n",
            "{'eval_loss': 3.611422061920166, 'eval_runtime': 149.542, 'eval_samples_per_second': 51.878, 'eval_steps_per_second': 6.486, 'epoch': 0.3620531622364803}\n",
            "{'loss': 3.9122, 'grad_norm': 0.9905009865760803, 'learning_rate': 4.392758936755271e-05, 'epoch': 0.36434463794683775}\n",
            "{'eval_loss': 3.6120338439941406, 'eval_runtime': 149.6265, 'eval_samples_per_second': 51.849, 'eval_steps_per_second': 6.483, 'epoch': 0.36434463794683775}\n",
            "{'loss': 3.9097, 'grad_norm': 1.141569972038269, 'learning_rate': 4.388939810571341e-05, 'epoch': 0.36663611365719523}\n",
            "{'eval_loss': 3.61195707321167, 'eval_runtime': 149.6161, 'eval_samples_per_second': 51.853, 'eval_steps_per_second': 6.483, 'epoch': 0.36663611365719523}\n",
            "{'loss': 3.8679, 'grad_norm': 1.0792149305343628, 'learning_rate': 4.3851206843874125e-05, 'epoch': 0.3689275893675527}\n",
            "{'eval_loss': 3.6122090816497803, 'eval_runtime': 149.6276, 'eval_samples_per_second': 51.849, 'eval_steps_per_second': 6.483, 'epoch': 0.3689275893675527}\n",
            "{'loss': 3.8751, 'grad_norm': 0.9954618811607361, 'learning_rate': 4.381301558203483e-05, 'epoch': 0.3712190650779102}\n",
            "{'eval_loss': 3.6120312213897705, 'eval_runtime': 149.4612, 'eval_samples_per_second': 51.906, 'eval_steps_per_second': 6.49, 'epoch': 0.3712190650779102}\n",
            "{'loss': 3.7555, 'grad_norm': 1.0556142330169678, 'learning_rate': 4.377482432019554e-05, 'epoch': 0.3735105407882676}\n",
            "{'eval_loss': 3.611255645751953, 'eval_runtime': 149.6299, 'eval_samples_per_second': 51.848, 'eval_steps_per_second': 6.483, 'epoch': 0.3735105407882676}\n",
            "{'loss': 3.8964, 'grad_norm': 1.2456828355789185, 'learning_rate': 4.3736633058356255e-05, 'epoch': 0.3758020164986251}\n",
            "{'eval_loss': 3.61043643951416, 'eval_runtime': 149.4579, 'eval_samples_per_second': 51.908, 'eval_steps_per_second': 6.49, 'epoch': 0.3758020164986251}\n",
            "{'loss': 3.8892, 'grad_norm': 1.0633244514465332, 'learning_rate': 4.369844179651696e-05, 'epoch': 0.3780934922089826}\n",
            "{'eval_loss': 3.6095590591430664, 'eval_runtime': 149.6512, 'eval_samples_per_second': 51.841, 'eval_steps_per_second': 6.482, 'epoch': 0.3780934922089826}\n",
            "{'loss': 3.8519, 'grad_norm': 0.9531048536300659, 'learning_rate': 4.3660250534677665e-05, 'epoch': 0.38038496791934007}\n",
            "{'eval_loss': 3.609485387802124, 'eval_runtime': 149.5244, 'eval_samples_per_second': 51.884, 'eval_steps_per_second': 6.487, 'epoch': 0.38038496791934007}\n",
            "{'loss': 3.7992, 'grad_norm': 1.0959906578063965, 'learning_rate': 4.362205927283838e-05, 'epoch': 0.38267644362969755}\n",
            "{'eval_loss': 3.609510898590088, 'eval_runtime': 149.6217, 'eval_samples_per_second': 51.851, 'eval_steps_per_second': 6.483, 'epoch': 0.38267644362969755}\n",
            "{'loss': 3.8893, 'grad_norm': 1.0012896060943604, 'learning_rate': 4.358386801099908e-05, 'epoch': 0.384967919340055}\n",
            "{'eval_loss': 3.609684705734253, 'eval_runtime': 149.5049, 'eval_samples_per_second': 51.891, 'eval_steps_per_second': 6.488, 'epoch': 0.384967919340055}\n",
            "{'loss': 3.9192, 'grad_norm': 1.3076146841049194, 'learning_rate': 4.3545676749159794e-05, 'epoch': 0.38725939505041246}\n",
            "{'eval_loss': 3.6092875003814697, 'eval_runtime': 149.5472, 'eval_samples_per_second': 51.877, 'eval_steps_per_second': 6.486, 'epoch': 0.38725939505041246}\n",
            "{'loss': 3.9352, 'grad_norm': 1.0919444561004639, 'learning_rate': 4.3507485487320506e-05, 'epoch': 0.38955087076076994}\n",
            "{'eval_loss': 3.609025716781616, 'eval_runtime': 149.5072, 'eval_samples_per_second': 51.89, 'eval_steps_per_second': 6.488, 'epoch': 0.38955087076076994}\n",
            "{'loss': 3.8489, 'grad_norm': 1.0048314332962036, 'learning_rate': 4.346929422548121e-05, 'epoch': 0.3918423464711274}\n",
            "{'eval_loss': 3.6087758541107178, 'eval_runtime': 149.4717, 'eval_samples_per_second': 51.903, 'eval_steps_per_second': 6.49, 'epoch': 0.3918423464711274}\n",
            "{'loss': 3.9431, 'grad_norm': 0.9416170716285706, 'learning_rate': 4.3431102963641924e-05, 'epoch': 0.39413382218148485}\n",
            "{'eval_loss': 3.6076741218566895, 'eval_runtime': 149.6203, 'eval_samples_per_second': 51.851, 'eval_steps_per_second': 6.483, 'epoch': 0.39413382218148485}\n",
            "{'loss': 3.8454, 'grad_norm': 0.9206799864768982, 'learning_rate': 4.339291170180263e-05, 'epoch': 0.39642529789184233}\n",
            "{'eval_loss': 3.6067512035369873, 'eval_runtime': 149.572, 'eval_samples_per_second': 51.868, 'eval_steps_per_second': 6.485, 'epoch': 0.39642529789184233}\n",
            "{'loss': 3.8472, 'grad_norm': 1.0409443378448486, 'learning_rate': 4.3354720439963334e-05, 'epoch': 0.3987167736021998}\n",
            "{'eval_loss': 3.606740713119507, 'eval_runtime': 149.4955, 'eval_samples_per_second': 51.895, 'eval_steps_per_second': 6.488, 'epoch': 0.3987167736021998}\n",
            "{'loss': 3.854, 'grad_norm': 1.1512748003005981, 'learning_rate': 4.3316529178124046e-05, 'epoch': 0.4010082493125573}\n",
            "{'eval_loss': 3.606287956237793, 'eval_runtime': 149.4317, 'eval_samples_per_second': 51.917, 'eval_steps_per_second': 6.491, 'epoch': 0.4010082493125573}\n",
            "{'loss': 3.8712, 'grad_norm': 0.9944533705711365, 'learning_rate': 4.327833791628476e-05, 'epoch': 0.4032997250229148}\n",
            "{'eval_loss': 3.6063263416290283, 'eval_runtime': 149.4597, 'eval_samples_per_second': 51.907, 'eval_steps_per_second': 6.49, 'epoch': 0.4032997250229148}\n",
            "{'loss': 3.8631, 'grad_norm': 1.1361924409866333, 'learning_rate': 4.3240146654445464e-05, 'epoch': 0.4055912007332722}\n",
            "{'eval_loss': 3.605890989303589, 'eval_runtime': 149.4457, 'eval_samples_per_second': 51.912, 'eval_steps_per_second': 6.491, 'epoch': 0.4055912007332722}\n",
            "{'loss': 3.97, 'grad_norm': 1.0466371774673462, 'learning_rate': 4.3201955392606176e-05, 'epoch': 0.4078826764436297}\n",
            "{'eval_loss': 3.6058287620544434, 'eval_runtime': 149.4423, 'eval_samples_per_second': 51.913, 'eval_steps_per_second': 6.491, 'epoch': 0.4078826764436297}\n",
            "{'loss': 3.9434, 'grad_norm': 0.999265730381012, 'learning_rate': 4.316376413076688e-05, 'epoch': 0.41017415215398717}\n",
            "{'eval_loss': 3.604891061782837, 'eval_runtime': 149.4526, 'eval_samples_per_second': 51.909, 'eval_steps_per_second': 6.49, 'epoch': 0.41017415215398717}\n",
            "{'loss': 3.9751, 'grad_norm': 0.9515755772590637, 'learning_rate': 4.312557286892759e-05, 'epoch': 0.41246562786434465}\n",
            "{'eval_loss': 3.6038925647735596, 'eval_runtime': 149.4907, 'eval_samples_per_second': 51.896, 'eval_steps_per_second': 6.489, 'epoch': 0.41246562786434465}\n",
            "{'loss': 3.8915, 'grad_norm': 1.0106974840164185, 'learning_rate': 4.30873816070883e-05, 'epoch': 0.41475710357470214}\n",
            "{'eval_loss': 3.6035726070404053, 'eval_runtime': 149.6408, 'eval_samples_per_second': 51.844, 'eval_steps_per_second': 6.482, 'epoch': 0.41475710357470214}\n",
            "{'loss': 3.9602, 'grad_norm': 0.9606088995933533, 'learning_rate': 4.304919034524901e-05, 'epoch': 0.41704857928505956}\n",
            "{'eval_loss': 3.603729724884033, 'eval_runtime': 149.5282, 'eval_samples_per_second': 51.883, 'eval_steps_per_second': 6.487, 'epoch': 0.41704857928505956}\n",
            "{'loss': 3.93, 'grad_norm': 0.9054582118988037, 'learning_rate': 4.301099908340972e-05, 'epoch': 0.41934005499541704}\n",
            "{'eval_loss': 3.6031763553619385, 'eval_runtime': 149.4536, 'eval_samples_per_second': 51.909, 'eval_steps_per_second': 6.49, 'epoch': 0.41934005499541704}\n",
            "{'loss': 3.8523, 'grad_norm': 1.0370497703552246, 'learning_rate': 4.297280782157043e-05, 'epoch': 0.4216315307057745}\n",
            "{'eval_loss': 3.6025381088256836, 'eval_runtime': 149.9475, 'eval_samples_per_second': 51.738, 'eval_steps_per_second': 6.469, 'epoch': 0.4216315307057745}\n",
            "{'loss': 3.9918, 'grad_norm': 1.0930614471435547, 'learning_rate': 4.293461655973113e-05, 'epoch': 0.423923006416132}\n",
            "{'eval_loss': 3.6018052101135254, 'eval_runtime': 150.1673, 'eval_samples_per_second': 51.662, 'eval_steps_per_second': 6.459, 'epoch': 0.423923006416132}\n",
            "{'loss': 3.9721, 'grad_norm': 0.9946860671043396, 'learning_rate': 4.2896425297891845e-05, 'epoch': 0.42621448212648944}\n",
            "{'eval_loss': 3.601224899291992, 'eval_runtime': 150.2981, 'eval_samples_per_second': 51.617, 'eval_steps_per_second': 6.454, 'epoch': 0.42621448212648944}\n",
            "{'loss': 3.8495, 'grad_norm': 0.9978758096694946, 'learning_rate': 4.285823403605255e-05, 'epoch': 0.4285059578368469}\n",
            "{'eval_loss': 3.600703239440918, 'eval_runtime': 150.1585, 'eval_samples_per_second': 51.665, 'eval_steps_per_second': 6.46, 'epoch': 0.4285059578368469}\n",
            "{'loss': 3.8762, 'grad_norm': 1.1305830478668213, 'learning_rate': 4.282004277421326e-05, 'epoch': 0.4307974335472044}\n",
            "{'eval_loss': 3.599891424179077, 'eval_runtime': 150.1419, 'eval_samples_per_second': 51.671, 'eval_steps_per_second': 6.461, 'epoch': 0.4307974335472044}\n",
            "{'loss': 3.9266, 'grad_norm': 0.8933533430099487, 'learning_rate': 4.2781851512373974e-05, 'epoch': 0.4330889092575619}\n",
            "{'eval_loss': 3.600001335144043, 'eval_runtime': 150.0774, 'eval_samples_per_second': 51.693, 'eval_steps_per_second': 6.463, 'epoch': 0.4330889092575619}\n",
            "{'loss': 3.8029, 'grad_norm': 0.9464012980461121, 'learning_rate': 4.274366025053468e-05, 'epoch': 0.43538038496791936}\n",
            "{'eval_loss': 3.600341558456421, 'eval_runtime': 150.2835, 'eval_samples_per_second': 51.622, 'eval_steps_per_second': 6.454, 'epoch': 0.43538038496791936}\n",
            "{'loss': 3.917, 'grad_norm': 0.8990715146064758, 'learning_rate': 4.270546898869539e-05, 'epoch': 0.4376718606782768}\n",
            "{'eval_loss': 3.6004743576049805, 'eval_runtime': 150.1075, 'eval_samples_per_second': 51.683, 'eval_steps_per_second': 6.462, 'epoch': 0.4376718606782768}\n",
            "{'loss': 3.8559, 'grad_norm': 0.901740312576294, 'learning_rate': 4.26672777268561e-05, 'epoch': 0.4399633363886343}\n",
            "{'eval_loss': 3.5999679565429688, 'eval_runtime': 150.1772, 'eval_samples_per_second': 51.659, 'eval_steps_per_second': 6.459, 'epoch': 0.4399633363886343}\n",
            "{'loss': 3.8816, 'grad_norm': 1.0408118963241577, 'learning_rate': 4.26290864650168e-05, 'epoch': 0.44225481209899176}\n",
            "{'eval_loss': 3.6001029014587402, 'eval_runtime': 150.1415, 'eval_samples_per_second': 51.671, 'eval_steps_per_second': 6.461, 'epoch': 0.44225481209899176}\n",
            "{'loss': 3.8707, 'grad_norm': 1.1571780443191528, 'learning_rate': 4.2590895203177514e-05, 'epoch': 0.44454628780934924}\n",
            "{'eval_loss': 3.600076675415039, 'eval_runtime': 150.3197, 'eval_samples_per_second': 51.61, 'eval_steps_per_second': 6.453, 'epoch': 0.44454628780934924}\n",
            "{'loss': 3.7454, 'grad_norm': 1.352548599243164, 'learning_rate': 4.2552703941338226e-05, 'epoch': 0.44683776351970667}\n",
            "{'eval_loss': 3.5994648933410645, 'eval_runtime': 150.2028, 'eval_samples_per_second': 51.65, 'eval_steps_per_second': 6.458, 'epoch': 0.44683776351970667}\n",
            "{'loss': 3.8663, 'grad_norm': 1.1134366989135742, 'learning_rate': 4.251451267949893e-05, 'epoch': 0.44912923923006415}\n",
            "{'eval_loss': 3.5985515117645264, 'eval_runtime': 150.2212, 'eval_samples_per_second': 51.644, 'eval_steps_per_second': 6.457, 'epoch': 0.44912923923006415}\n",
            "{'loss': 3.8537, 'grad_norm': 0.9486895203590393, 'learning_rate': 4.247632141765964e-05, 'epoch': 0.45142071494042163}\n",
            "{'eval_loss': 3.5973379611968994, 'eval_runtime': 150.2342, 'eval_samples_per_second': 51.639, 'eval_steps_per_second': 6.457, 'epoch': 0.45142071494042163}\n",
            "{'loss': 3.867, 'grad_norm': 0.9997326731681824, 'learning_rate': 4.243813015582035e-05, 'epoch': 0.4537121906507791}\n",
            "{'eval_loss': 3.596714735031128, 'eval_runtime': 150.2533, 'eval_samples_per_second': 51.633, 'eval_steps_per_second': 6.456, 'epoch': 0.4537121906507791}\n",
            "{'loss': 3.8865, 'grad_norm': 0.9789434671401978, 'learning_rate': 4.239993889398106e-05, 'epoch': 0.4560036663611366}\n",
            "{'eval_loss': 3.596127986907959, 'eval_runtime': 150.3068, 'eval_samples_per_second': 51.614, 'eval_steps_per_second': 6.453, 'epoch': 0.4560036663611366}\n",
            "{'loss': 3.9189, 'grad_norm': 0.9855333566665649, 'learning_rate': 4.2361747632141766e-05, 'epoch': 0.458295142071494}\n",
            "{'eval_loss': 3.5961432456970215, 'eval_runtime': 150.2685, 'eval_samples_per_second': 51.628, 'eval_steps_per_second': 6.455, 'epoch': 0.458295142071494}\n",
            "{'loss': 3.948, 'grad_norm': 1.017500638961792, 'learning_rate': 4.232355637030248e-05, 'epoch': 0.4605866177818515}\n",
            "{'eval_loss': 3.5962958335876465, 'eval_runtime': 150.4024, 'eval_samples_per_second': 51.582, 'eval_steps_per_second': 6.449, 'epoch': 0.4605866177818515}\n",
            "{'loss': 3.9249, 'grad_norm': 1.145099401473999, 'learning_rate': 4.228536510846319e-05, 'epoch': 0.462878093492209}\n",
            "{'eval_loss': 3.5962867736816406, 'eval_runtime': 150.2874, 'eval_samples_per_second': 51.621, 'eval_steps_per_second': 6.454, 'epoch': 0.462878093492209}\n",
            "{'loss': 3.9111, 'grad_norm': 1.1027828454971313, 'learning_rate': 4.2247173846623895e-05, 'epoch': 0.46516956920256647}\n",
            "{'eval_loss': 3.595846176147461, 'eval_runtime': 150.2155, 'eval_samples_per_second': 51.646, 'eval_steps_per_second': 6.457, 'epoch': 0.46516956920256647}\n",
            "{'loss': 3.8271, 'grad_norm': 1.0754841566085815, 'learning_rate': 4.22089825847846e-05, 'epoch': 0.4674610449129239}\n",
            "{'eval_loss': 3.596153736114502, 'eval_runtime': 150.4294, 'eval_samples_per_second': 51.572, 'eval_steps_per_second': 6.448, 'epoch': 0.4674610449129239}\n",
            "{'loss': 3.8966, 'grad_norm': 0.9991546869277954, 'learning_rate': 4.217079132294531e-05, 'epoch': 0.4697525206232814}\n",
            "{'eval_loss': 3.5967187881469727, 'eval_runtime': 150.3381, 'eval_samples_per_second': 51.604, 'eval_steps_per_second': 6.452, 'epoch': 0.4697525206232814}\n",
            "{'loss': 3.8866, 'grad_norm': 1.0009845495224, 'learning_rate': 4.213260006110602e-05, 'epoch': 0.47204399633363886}\n",
            "{'eval_loss': 3.597149133682251, 'eval_runtime': 150.4694, 'eval_samples_per_second': 51.559, 'eval_steps_per_second': 6.446, 'epoch': 0.47204399633363886}\n",
            "{'loss': 3.9938, 'grad_norm': 1.0381450653076172, 'learning_rate': 4.209440879926673e-05, 'epoch': 0.47433547204399634}\n",
            "{'eval_loss': 3.596982479095459, 'eval_runtime': 150.4561, 'eval_samples_per_second': 51.563, 'eval_steps_per_second': 6.447, 'epoch': 0.47433547204399634}\n",
            "{'loss': 3.8694, 'grad_norm': 1.0067265033721924, 'learning_rate': 4.205621753742744e-05, 'epoch': 0.4766269477543538}\n",
            "{'eval_loss': 3.5963215827941895, 'eval_runtime': 150.3889, 'eval_samples_per_second': 51.586, 'eval_steps_per_second': 6.45, 'epoch': 0.4766269477543538}\n",
            "{'loss': 3.9671, 'grad_norm': 0.9098036289215088, 'learning_rate': 4.201802627558815e-05, 'epoch': 0.47891842346471125}\n",
            "{'eval_loss': 3.595712184906006, 'eval_runtime': 150.49, 'eval_samples_per_second': 51.552, 'eval_steps_per_second': 6.446, 'epoch': 0.47891842346471125}\n",
            "{'loss': 3.8588, 'grad_norm': 1.0529637336730957, 'learning_rate': 4.197983501374886e-05, 'epoch': 0.48120989917506873}\n",
            "{'eval_loss': 3.595386505126953, 'eval_runtime': 150.4254, 'eval_samples_per_second': 51.574, 'eval_steps_per_second': 6.448, 'epoch': 0.48120989917506873}\n",
            "{'loss': 3.8302, 'grad_norm': 1.037841796875, 'learning_rate': 4.1941643751909564e-05, 'epoch': 0.4835013748854262}\n",
            "{'eval_loss': 3.594435214996338, 'eval_runtime': 150.4188, 'eval_samples_per_second': 51.576, 'eval_steps_per_second': 6.449, 'epoch': 0.4835013748854262}\n",
            "{'loss': 3.9611, 'grad_norm': 1.122079849243164, 'learning_rate': 4.190345249007027e-05, 'epoch': 0.4857928505957837}\n",
            "{'eval_loss': 3.594407081604004, 'eval_runtime': 150.4482, 'eval_samples_per_second': 51.566, 'eval_steps_per_second': 6.447, 'epoch': 0.4857928505957837}\n",
            "{'loss': 3.8872, 'grad_norm': 1.208909034729004, 'learning_rate': 4.186526122823098e-05, 'epoch': 0.4880843263061412}\n",
            "{'eval_loss': 3.5941951274871826, 'eval_runtime': 150.2971, 'eval_samples_per_second': 51.618, 'eval_steps_per_second': 6.454, 'epoch': 0.4880843263061412}\n",
            "{'loss': 3.8583, 'grad_norm': 1.0320881605148315, 'learning_rate': 4.1827069966391694e-05, 'epoch': 0.4903758020164986}\n",
            "{'eval_loss': 3.5942041873931885, 'eval_runtime': 150.3403, 'eval_samples_per_second': 51.603, 'eval_steps_per_second': 6.452, 'epoch': 0.4903758020164986}\n",
            "{'loss': 3.8518, 'grad_norm': 0.9102158546447754, 'learning_rate': 4.17888787045524e-05, 'epoch': 0.4926672777268561}\n",
            "{'eval_loss': 3.593761682510376, 'eval_runtime': 150.2839, 'eval_samples_per_second': 51.622, 'eval_steps_per_second': 6.454, 'epoch': 0.4926672777268561}\n",
            "{'loss': 3.8969, 'grad_norm': 1.0926569700241089, 'learning_rate': 4.175068744271311e-05, 'epoch': 0.49495875343721357}\n",
            "{'eval_loss': 3.5930793285369873, 'eval_runtime': 150.4889, 'eval_samples_per_second': 51.552, 'eval_steps_per_second': 6.446, 'epoch': 0.49495875343721357}\n",
            "{'loss': 3.9516, 'grad_norm': 0.9202452898025513, 'learning_rate': 4.1712496180873816e-05, 'epoch': 0.49725022914757105}\n",
            "{'eval_loss': 3.592376470565796, 'eval_runtime': 150.149, 'eval_samples_per_second': 51.669, 'eval_steps_per_second': 6.46, 'epoch': 0.49725022914757105}\n",
            "{'loss': 3.9341, 'grad_norm': 1.1125950813293457, 'learning_rate': 4.167430491903453e-05, 'epoch': 0.4995417048579285}\n",
            "{'eval_loss': 3.5926473140716553, 'eval_runtime': 150.1564, 'eval_samples_per_second': 51.666, 'eval_steps_per_second': 6.46, 'epoch': 0.4995417048579285}\n",
            "{'loss': 3.9384, 'grad_norm': 1.1478780508041382, 'learning_rate': 4.1636113657195234e-05, 'epoch': 0.501833180568286}\n",
            "{'eval_loss': 3.592405319213867, 'eval_runtime': 150.2848, 'eval_samples_per_second': 51.622, 'eval_steps_per_second': 6.454, 'epoch': 0.501833180568286}\n",
            "{'loss': 3.8435, 'grad_norm': 1.072212815284729, 'learning_rate': 4.1597922395355946e-05, 'epoch': 0.5041246562786434}\n",
            "{'eval_loss': 3.5913710594177246, 'eval_runtime': 150.4424, 'eval_samples_per_second': 51.568, 'eval_steps_per_second': 6.448, 'epoch': 0.5041246562786434}\n",
            "{'loss': 3.9149, 'grad_norm': 0.8810800313949585, 'learning_rate': 4.155973113351666e-05, 'epoch': 0.5064161319890009}\n",
            "{'eval_loss': 3.5907702445983887, 'eval_runtime': 150.3016, 'eval_samples_per_second': 51.616, 'eval_steps_per_second': 6.454, 'epoch': 0.5064161319890009}\n",
            "{'loss': 3.9253, 'grad_norm': 0.9653258323669434, 'learning_rate': 4.152153987167736e-05, 'epoch': 0.5087076076993584}\n",
            "{'eval_loss': 3.5904252529144287, 'eval_runtime': 150.303, 'eval_samples_per_second': 51.616, 'eval_steps_per_second': 6.454, 'epoch': 0.5087076076993584}\n",
            "{'loss': 3.8421, 'grad_norm': 0.8235311508178711, 'learning_rate': 4.148334860983807e-05, 'epoch': 0.5109990834097159}\n",
            "{'eval_loss': 3.590308427810669, 'eval_runtime': 150.3425, 'eval_samples_per_second': 51.602, 'eval_steps_per_second': 6.452, 'epoch': 0.5109990834097159}\n",
            "{'loss': 3.8943, 'grad_norm': 1.0846970081329346, 'learning_rate': 4.144515734799878e-05, 'epoch': 0.5132905591200734}\n",
            "{'eval_loss': 3.5905935764312744, 'eval_runtime': 150.1848, 'eval_samples_per_second': 51.656, 'eval_steps_per_second': 6.459, 'epoch': 0.5132905591200734}\n",
            "{'loss': 3.9406, 'grad_norm': 1.0688987970352173, 'learning_rate': 4.1406966086159485e-05, 'epoch': 0.5155820348304307}\n",
            "{'eval_loss': 3.5908093452453613, 'eval_runtime': 150.344, 'eval_samples_per_second': 51.602, 'eval_steps_per_second': 6.452, 'epoch': 0.5155820348304307}\n",
            "{'loss': 3.9079, 'grad_norm': 0.9875842928886414, 'learning_rate': 4.13687748243202e-05, 'epoch': 0.5178735105407882}\n",
            "{'eval_loss': 3.5910909175872803, 'eval_runtime': 150.3247, 'eval_samples_per_second': 51.608, 'eval_steps_per_second': 6.453, 'epoch': 0.5178735105407882}\n",
            "{'loss': 3.934, 'grad_norm': 1.0512043237686157, 'learning_rate': 4.133058356248091e-05, 'epoch': 0.5201649862511457}\n",
            "{'eval_loss': 3.590421676635742, 'eval_runtime': 150.1703, 'eval_samples_per_second': 51.661, 'eval_steps_per_second': 6.459, 'epoch': 0.5201649862511457}\n",
            "{'loss': 3.8224, 'grad_norm': 0.9425788521766663, 'learning_rate': 4.1292392300641615e-05, 'epoch': 0.5224564619615032}\n",
            "{'eval_loss': 3.589529514312744, 'eval_runtime': 150.1328, 'eval_samples_per_second': 51.674, 'eval_steps_per_second': 6.461, 'epoch': 0.5224564619615032}\n",
            "{'loss': 4.0627, 'grad_norm': 1.0423213243484497, 'learning_rate': 4.125420103880233e-05, 'epoch': 0.5247479376718607}\n",
            "{'eval_loss': 3.588827133178711, 'eval_runtime': 150.2227, 'eval_samples_per_second': 51.643, 'eval_steps_per_second': 6.457, 'epoch': 0.5247479376718607}\n",
            "{'loss': 3.899, 'grad_norm': 1.1905564069747925, 'learning_rate': 4.121600977696303e-05, 'epoch': 0.5270394133822182}\n",
            "{'eval_loss': 3.587871551513672, 'eval_runtime': 149.9722, 'eval_samples_per_second': 51.73, 'eval_steps_per_second': 6.468, 'epoch': 0.5270394133822182}\n",
            "{'loss': 3.8723, 'grad_norm': 0.943768322467804, 'learning_rate': 4.117781851512374e-05, 'epoch': 0.5293308890925756}\n",
            "{'eval_loss': 3.587327718734741, 'eval_runtime': 150.1235, 'eval_samples_per_second': 51.677, 'eval_steps_per_second': 6.461, 'epoch': 0.5293308890925756}\n",
            "{'loss': 3.8521, 'grad_norm': 1.0017021894454956, 'learning_rate': 4.113962725328445e-05, 'epoch': 0.5316223648029331}\n",
            "{'eval_loss': 3.5873045921325684, 'eval_runtime': 150.2819, 'eval_samples_per_second': 51.623, 'eval_steps_per_second': 6.455, 'epoch': 0.5316223648029331}\n",
            "{'loss': 3.9517, 'grad_norm': 1.2758651971817017, 'learning_rate': 4.110143599144516e-05, 'epoch': 0.5339138405132906}\n",
            "{'eval_loss': 3.587279796600342, 'eval_runtime': 150.0556, 'eval_samples_per_second': 51.701, 'eval_steps_per_second': 6.464, 'epoch': 0.5339138405132906}\n",
            "{'loss': 3.9384, 'grad_norm': 1.0552746057510376, 'learning_rate': 4.1063244729605873e-05, 'epoch': 0.536205316223648}\n",
            "{'eval_loss': 3.587165117263794, 'eval_runtime': 150.1951, 'eval_samples_per_second': 51.653, 'eval_steps_per_second': 6.458, 'epoch': 0.536205316223648}\n",
            "{'loss': 3.8898, 'grad_norm': 0.9666781425476074, 'learning_rate': 4.102505346776658e-05, 'epoch': 0.5384967919340055}\n",
            "{'eval_loss': 3.5874104499816895, 'eval_runtime': 150.1307, 'eval_samples_per_second': 51.675, 'eval_steps_per_second': 6.461, 'epoch': 0.5384967919340055}\n",
            "{'loss': 3.9795, 'grad_norm': 1.124692440032959, 'learning_rate': 4.0986862205927284e-05, 'epoch': 0.5407882676443629}\n",
            "{'eval_loss': 3.5878829956054688, 'eval_runtime': 150.0169, 'eval_samples_per_second': 51.714, 'eval_steps_per_second': 6.466, 'epoch': 0.5407882676443629}\n",
            "{'loss': 3.9324, 'grad_norm': 1.0161782503128052, 'learning_rate': 4.0948670944087996e-05, 'epoch': 0.5430797433547204}\n",
            "{'eval_loss': 3.5875940322875977, 'eval_runtime': 150.1372, 'eval_samples_per_second': 51.673, 'eval_steps_per_second': 6.461, 'epoch': 0.5430797433547204}\n",
            "{'loss': 3.9075, 'grad_norm': 0.918635904788971, 'learning_rate': 4.09104796822487e-05, 'epoch': 0.5453712190650779}\n",
            "{'eval_loss': 3.587202548980713, 'eval_runtime': 150.1652, 'eval_samples_per_second': 51.663, 'eval_steps_per_second': 6.46, 'epoch': 0.5453712190650779}\n",
            "{'loss': 3.9514, 'grad_norm': 0.9970774054527283, 'learning_rate': 4.087228842040941e-05, 'epoch': 0.5476626947754354}\n",
            "{'eval_loss': 3.5868425369262695, 'eval_runtime': 150.106, 'eval_samples_per_second': 51.683, 'eval_steps_per_second': 6.462, 'epoch': 0.5476626947754354}\n",
            "{'loss': 3.8223, 'grad_norm': 1.228041410446167, 'learning_rate': 4.0834097158570125e-05, 'epoch': 0.5499541704857929}\n",
            "{'eval_loss': 3.586385726928711, 'eval_runtime': 150.0202, 'eval_samples_per_second': 51.713, 'eval_steps_per_second': 6.466, 'epoch': 0.5499541704857929}\n",
            "{'loss': 3.872, 'grad_norm': 0.9618282914161682, 'learning_rate': 4.079590589673083e-05, 'epoch': 0.5522456461961504}\n",
            "{'eval_loss': 3.585866928100586, 'eval_runtime': 149.9618, 'eval_samples_per_second': 51.733, 'eval_steps_per_second': 6.468, 'epoch': 0.5522456461961504}\n",
            "{'loss': 3.8405, 'grad_norm': 1.0390570163726807, 'learning_rate': 4.0757714634891536e-05, 'epoch': 0.5545371219065078}\n",
            "{'eval_loss': 3.586258888244629, 'eval_runtime': 149.7628, 'eval_samples_per_second': 51.802, 'eval_steps_per_second': 6.477, 'epoch': 0.5545371219065078}\n",
            "{'loss': 3.7334, 'grad_norm': 0.9786362051963806, 'learning_rate': 4.071952337305225e-05, 'epoch': 0.5568285976168652}\n",
            "{'eval_loss': 3.5865118503570557, 'eval_runtime': 149.7988, 'eval_samples_per_second': 51.789, 'eval_steps_per_second': 6.475, 'epoch': 0.5568285976168652}\n",
            "{'loss': 3.9582, 'grad_norm': 1.190887212753296, 'learning_rate': 4.068133211121295e-05, 'epoch': 0.5591200733272227}\n",
            "{'eval_loss': 3.586372137069702, 'eval_runtime': 149.7235, 'eval_samples_per_second': 51.816, 'eval_steps_per_second': 6.479, 'epoch': 0.5591200733272227}\n",
            "{'loss': 3.8074, 'grad_norm': 1.0418461561203003, 'learning_rate': 4.0643140849373665e-05, 'epoch': 0.5614115490375802}\n",
            "{'eval_loss': 3.5859265327453613, 'eval_runtime': 149.7427, 'eval_samples_per_second': 51.809, 'eval_steps_per_second': 6.478, 'epoch': 0.5614115490375802}\n",
            "{'loss': 3.8036, 'grad_norm': 1.1537343263626099, 'learning_rate': 4.060494958753438e-05, 'epoch': 0.5637030247479377}\n",
            "{'eval_loss': 3.58571457862854, 'eval_runtime': 149.8694, 'eval_samples_per_second': 51.765, 'eval_steps_per_second': 6.472, 'epoch': 0.5637030247479377}\n",
            "{'loss': 3.862, 'grad_norm': 1.1392006874084473, 'learning_rate': 4.056675832569508e-05, 'epoch': 0.5659945004582951}\n",
            "{'eval_loss': 3.586043119430542, 'eval_runtime': 149.7583, 'eval_samples_per_second': 51.803, 'eval_steps_per_second': 6.477, 'epoch': 0.5659945004582951}\n",
            "{'loss': 3.9441, 'grad_norm': 1.116569995880127, 'learning_rate': 4.0528567063855795e-05, 'epoch': 0.5682859761686526}\n",
            "{'eval_loss': 3.5861127376556396, 'eval_runtime': 149.7962, 'eval_samples_per_second': 51.79, 'eval_steps_per_second': 6.475, 'epoch': 0.5682859761686526}\n",
            "{'loss': 3.8521, 'grad_norm': 1.0715460777282715, 'learning_rate': 4.04903758020165e-05, 'epoch': 0.5705774518790101}\n",
            "{'eval_loss': 3.5864882469177246, 'eval_runtime': 149.7426, 'eval_samples_per_second': 51.809, 'eval_steps_per_second': 6.478, 'epoch': 0.5705774518790101}\n",
            "{'loss': 3.7776, 'grad_norm': 0.94674152135849, 'learning_rate': 4.0452184540177205e-05, 'epoch': 0.5728689275893676}\n",
            "{'eval_loss': 3.586827278137207, 'eval_runtime': 150.3661, 'eval_samples_per_second': 51.594, 'eval_steps_per_second': 6.451, 'epoch': 0.5728689275893676}\n",
            "{'loss': 3.8969, 'grad_norm': 0.8731008172035217, 'learning_rate': 4.041399327833792e-05, 'epoch': 0.5751604032997251}\n",
            "{'eval_loss': 3.586925983428955, 'eval_runtime': 150.3596, 'eval_samples_per_second': 51.596, 'eval_steps_per_second': 6.451, 'epoch': 0.5751604032997251}\n",
            "{'loss': 3.9552, 'grad_norm': 1.0761133432388306, 'learning_rate': 4.037580201649863e-05, 'epoch': 0.5774518790100825}\n",
            "{'eval_loss': 3.587301015853882, 'eval_runtime': 150.0945, 'eval_samples_per_second': 51.687, 'eval_steps_per_second': 6.463, 'epoch': 0.5774518790100825}\n",
            "{'loss': 3.9178, 'grad_norm': 1.1139799356460571, 'learning_rate': 4.033761075465934e-05, 'epoch': 0.5797433547204399}\n",
            "{'eval_loss': 3.586904287338257, 'eval_runtime': 150.1501, 'eval_samples_per_second': 51.668, 'eval_steps_per_second': 6.46, 'epoch': 0.5797433547204399}\n",
            "{'loss': 3.9139, 'grad_norm': 0.8968650698661804, 'learning_rate': 4.0299419492820046e-05, 'epoch': 0.5820348304307974}\n",
            "{'eval_loss': 3.5864248275756836, 'eval_runtime': 150.1789, 'eval_samples_per_second': 51.658, 'eval_steps_per_second': 6.459, 'epoch': 0.5820348304307974}\n",
            "{'loss': 3.8271, 'grad_norm': 1.026672124862671, 'learning_rate': 4.026122823098075e-05, 'epoch': 0.5843263061411549}\n",
            "{'eval_loss': 3.5860140323638916, 'eval_runtime': 150.1006, 'eval_samples_per_second': 51.685, 'eval_steps_per_second': 6.462, 'epoch': 0.5843263061411549}\n",
            "{'loss': 3.8983, 'grad_norm': 1.1643123626708984, 'learning_rate': 4.0223036969141464e-05, 'epoch': 0.5866177818515124}\n",
            "{'eval_loss': 3.5854947566986084, 'eval_runtime': 150.2171, 'eval_samples_per_second': 51.645, 'eval_steps_per_second': 6.457, 'epoch': 0.5866177818515124}\n",
            "{'loss': 3.8717, 'grad_norm': 1.2345255613327026, 'learning_rate': 4.018484570730217e-05, 'epoch': 0.5889092575618698}\n",
            "{'eval_loss': 3.585146903991699, 'eval_runtime': 150.2682, 'eval_samples_per_second': 51.628, 'eval_steps_per_second': 6.455, 'epoch': 0.5889092575618698}\n",
            "{'loss': 3.8404, 'grad_norm': 0.9776536822319031, 'learning_rate': 4.014665444546288e-05, 'epoch': 0.5912007332722273}\n",
            "{'eval_loss': 3.585191488265991, 'eval_runtime': 150.3128, 'eval_samples_per_second': 51.612, 'eval_steps_per_second': 6.453, 'epoch': 0.5912007332722273}\n",
            "{'loss': 3.9275, 'grad_norm': 1.0781610012054443, 'learning_rate': 4.010846318362359e-05, 'epoch': 0.5934922089825848}\n",
            "{'eval_loss': 3.5850417613983154, 'eval_runtime': 150.1216, 'eval_samples_per_second': 51.678, 'eval_steps_per_second': 6.461, 'epoch': 0.5934922089825848}\n",
            "{'loss': 3.86, 'grad_norm': 1.1210521459579468, 'learning_rate': 4.00702719217843e-05, 'epoch': 0.5957836846929423}\n",
            "{'eval_loss': 3.5847582817077637, 'eval_runtime': 150.151, 'eval_samples_per_second': 51.668, 'eval_steps_per_second': 6.46, 'epoch': 0.5957836846929423}\n",
            "{'loss': 3.9861, 'grad_norm': 1.041433334350586, 'learning_rate': 4.0032080659945004e-05, 'epoch': 0.5980751604032998}\n",
            "{'eval_loss': 3.5844037532806396, 'eval_runtime': 150.0548, 'eval_samples_per_second': 51.701, 'eval_steps_per_second': 6.464, 'epoch': 0.5980751604032998}\n",
            "{'loss': 3.8411, 'grad_norm': 0.8584718108177185, 'learning_rate': 3.9993889398105716e-05, 'epoch': 0.6003666361136571}\n",
            "{'eval_loss': 3.5838189125061035, 'eval_runtime': 150.1015, 'eval_samples_per_second': 51.685, 'eval_steps_per_second': 6.462, 'epoch': 0.6003666361136571}\n",
            "{'loss': 3.7969, 'grad_norm': 1.0275801420211792, 'learning_rate': 3.995569813626642e-05, 'epoch': 0.6026581118240146}\n",
            "{'eval_loss': 3.583669900894165, 'eval_runtime': 150.1127, 'eval_samples_per_second': 51.681, 'eval_steps_per_second': 6.462, 'epoch': 0.6026581118240146}\n",
            "{'loss': 3.9876, 'grad_norm': 1.051066279411316, 'learning_rate': 3.991750687442713e-05, 'epoch': 0.6049495875343721}\n",
            "{'eval_loss': 3.5827019214630127, 'eval_runtime': 150.2658, 'eval_samples_per_second': 51.629, 'eval_steps_per_second': 6.455, 'epoch': 0.6049495875343721}\n",
            "{'loss': 3.925, 'grad_norm': 1.031868815422058, 'learning_rate': 3.9879315612587845e-05, 'epoch': 0.6072410632447296}\n",
            "{'eval_loss': 3.5822136402130127, 'eval_runtime': 150.2264, 'eval_samples_per_second': 51.642, 'eval_steps_per_second': 6.457, 'epoch': 0.6072410632447296}\n",
            "{'loss': 3.9664, 'grad_norm': 0.929932177066803, 'learning_rate': 3.984112435074855e-05, 'epoch': 0.6095325389550871}\n",
            "{'eval_loss': 3.581928014755249, 'eval_runtime': 150.2062, 'eval_samples_per_second': 51.649, 'eval_steps_per_second': 6.458, 'epoch': 0.6095325389550871}\n",
            "{'loss': 3.8792, 'grad_norm': 1.2552143335342407, 'learning_rate': 3.980293308890926e-05, 'epoch': 0.6118240146654446}\n",
            "{'eval_loss': 3.58223557472229, 'eval_runtime': 150.2906, 'eval_samples_per_second': 51.62, 'eval_steps_per_second': 6.454, 'epoch': 0.6118240146654446}\n",
            "{'loss': 3.8861, 'grad_norm': 1.034362554550171, 'learning_rate': 3.976474182706997e-05, 'epoch': 0.614115490375802}\n",
            "{'eval_loss': 3.582908868789673, 'eval_runtime': 150.1805, 'eval_samples_per_second': 51.658, 'eval_steps_per_second': 6.459, 'epoch': 0.614115490375802}\n",
            "{'loss': 3.9467, 'grad_norm': 0.980675220489502, 'learning_rate': 3.972655056523067e-05, 'epoch': 0.6164069660861595}\n",
            "{'eval_loss': 3.58263897895813, 'eval_runtime': 150.2156, 'eval_samples_per_second': 51.646, 'eval_steps_per_second': 6.457, 'epoch': 0.6164069660861595}\n",
            "{'loss': 3.8144, 'grad_norm': 1.0196053981781006, 'learning_rate': 3.9688359303391385e-05, 'epoch': 0.618698441796517}\n",
            "{'eval_loss': 3.5829436779022217, 'eval_runtime': 150.1834, 'eval_samples_per_second': 51.657, 'eval_steps_per_second': 6.459, 'epoch': 0.618698441796517}\n",
            "{'loss': 3.9436, 'grad_norm': 1.0761569738388062, 'learning_rate': 3.96501680415521e-05, 'epoch': 0.6209899175068744}\n",
            "{'eval_loss': 3.5832018852233887, 'eval_runtime': 150.0653, 'eval_samples_per_second': 51.697, 'eval_steps_per_second': 6.464, 'epoch': 0.6209899175068744}\n",
            "{'loss': 3.8387, 'grad_norm': 0.9656322598457336, 'learning_rate': 3.961197677971281e-05, 'epoch': 0.6232813932172319}\n",
            "{'eval_loss': 3.5826103687286377, 'eval_runtime': 150.1875, 'eval_samples_per_second': 51.655, 'eval_steps_per_second': 6.459, 'epoch': 0.6232813932172319}\n",
            "{'loss': 3.8529, 'grad_norm': 1.0411180257797241, 'learning_rate': 3.9573785517873514e-05, 'epoch': 0.6255728689275893}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# .to() doesn't accept non_blocking as kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BatchEncoding.to() got an unexpected keyword argument 'non_blocking'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b0cdc5399a1d>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Start fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2343\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2791\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2793\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2750\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3640\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3641\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3642\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3643\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3815\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3816\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3817\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3818\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;31m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# .to() doesn't accept non_blocking as kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, save the model and tokenizer\n",
        "trainer.save_model(\"./gptNYT-2016-2022\")\n",
        "tokenizer.save_pretrained(\"./gptNYT-2016-2022\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "menomhtWviLi",
        "outputId": "4f1525a8-152f-4b15-bd6d-3fb7d1527b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gptNYT-2016-2022/tokenizer_config.json',\n",
              " './gptNYT-2016-2022/special_tokens_map.json',\n",
              " './gptNYT-2016-2022/vocab.json',\n",
              " './gptNYT-2016-2022/merges.txt',\n",
              " './gptNYT-2016-2022/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Path to the directory containing the model and tokenizer\n",
        "model_directory = \"./gptNYT-2016-2022\"\n",
        "\n",
        "# Path to save the zipped file\n",
        "output_filename = \"./gptNYT-2016-2022.zip\"\n",
        "\n",
        "# Zip the directory\n",
        "shutil.make_archive(base_name=output_filename.replace('.zip', ''), format='zip', root_dir=model_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Of_qwqEt7cEa",
        "outputId": "66225c28-814b-497b-f543-438c56885656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gptNYT-2016-2022.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "pretrained_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "fine_tuned_model_path = \"./my_finetuned_model\"\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path).to(device)\n",
        "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Function to generate text with attention mask\n",
        "def generate_text(model, tokenizer, prompt, max_length=100):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones(inputs.shape, device=device)  # Create attention mask\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,  # Pass attention mask\n",
        "        max_length=max_length,\n",
        "        pad_token_id=tokenizer.eos_token_id  # Set pad token ID to eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Testing the pre-trained model\n",
        "pretrained_prompt = \"In 2021\"\n",
        "pretrained_output = generate_text(pretrained_model, tokenizer, pretrained_prompt)\n",
        "print(\"Pre-trained Model Output:\")\n",
        "print(pretrained_output)\n",
        "\n",
        "# Testing the fine-tuned model\n",
        "finetuned_prompt = \"In 2021\"\n",
        "finetuned_output = generate_text(fine_tuned_model, fine_tuned_tokenizer, finetuned_prompt)\n",
        "print(\"\\nFine-tuned Model Output:\")\n",
        "print(finetuned_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5QveU6cvl0A",
        "outputId": "e5955a63-065a-4d7c-c055-149dbdde04f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained Model Output:\n",
            "In 2021, the government will have to decide whether to extend the existing contract with the company.\n",
            "\n",
            "The government has already announced that it will not renew the contract with the company.\n",
            "\n",
            "The government has also announced that it will not renew the contract with the company.\n",
            "\n",
            "The government has also announced that it will not renew the contract with the company.\n",
            "\n",
            "The government has also announced that it will not renew the contract with the company.\n",
            "\n",
            "The government has also announced that it\n",
            "\n",
            "Fine-tuned Model Output:\n",
            "In 2021, the company will begin to sell its first-generation electric vehicles, which will be powered by a battery pack. The company will also offer a range of other products, including a range of smart home devices. The company is also developing a new product that will allow users to control their own home. The company is also developing a new product that will allow users to control their own home. The company is also developing a new product that will allow users to control their own home. The company\n"
          ]
        }
      ]
    }
  ]
}